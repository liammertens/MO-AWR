{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":130579,"status":"ok","timestamp":1749365972819,"user":{"displayName":"Liam Mertens","userId":"17654526473594897979"},"user_tz":-120},"id":"z_jVbzLTRMFu","outputId":"6bbf5461-fc5b-4639-88d0-5c79d9c505f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mo-gymnasium\n","  Downloading mo_gymnasium-1.3.1-py3-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: gymnasium>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from mo-gymnasium) (1.1.1)\n","Collecting numpy<2.0,>=1.21.0 (from mo-gymnasium)\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from mo-gymnasium) (2.6.1)\n","Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.11/dist-packages (from mo-gymnasium) (1.15.3)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->mo-gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->mo-gymnasium) (4.14.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->mo-gymnasium) (0.0.4)\n","Downloading mo_gymnasium-1.3.1-py3-none-any.whl (479 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m479.6/479.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy, mo-gymnasium\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed mo-gymnasium-1.3.1 numpy-1.26.4\n","Collecting git+https://github.com/LucasAlegre/morl-baselines.git\n","  Cloning https://github.com/LucasAlegre/morl-baselines.git to /tmp/pip-req-build-smzv99ad\n","  Running command git clone --filter=blob:none --quiet https://github.com/LucasAlegre/morl-baselines.git /tmp/pip-req-build-smzv99ad\n","  Resolved https://github.com/LucasAlegre/morl-baselines.git to commit 92572aa5ee13e98b9e080f64df8e29602dba4f69\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: mo-gymnasium>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from morl-baselines==1.1.0) (1.3.1)\n","Requirement already satisfied: gymnasium>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from morl-baselines==1.1.0) (1.1.1)\n","Requirement already satisfied: numpy<2.0.0,>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from morl-baselines==1.1.0) (1.26.4)\n","Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from morl-baselines==1.1.0) (2.6.0+cu124)\n","Requirement already satisfied: pygame>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from morl-baselines==1.1.0) (2.6.1)\n","Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.11/dist-packages (from morl-baselines==1.1.0) (1.15.3)\n","Collecting pymoo>=0.6.0 (from morl-baselines==1.1.0)\n","  Downloading pymoo-0.6.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: wandb>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from morl-baselines==1.1.0) (0.19.11)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from morl-baselines==1.1.0) (2.37.0)\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (from morl-baselines==1.1.0) (1.0.3)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from morl-baselines==1.1.0) (0.13.2)\n","Requirement already satisfied: cvxpy in /usr/local/lib/python3.11/dist-packages (from morl-baselines==1.1.0) (1.6.5)\n","Collecting fire (from morl-baselines==1.1.0)\n","  Downloading fire-0.7.0.tar.gz (87 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->morl-baselines==1.1.0) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->morl-baselines==1.1.0) (4.14.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->morl-baselines==1.1.0) (0.0.4)\n","Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.11/dist-packages (from pymoo>=0.6.0->morl-baselines==1.1.0) (3.10.0)\n","Requirement already satisfied: autograd>=1.4 in /usr/local/lib/python3.11/dist-packages (from pymoo>=0.6.0->morl-baselines==1.1.0) (1.8.0)\n","Collecting cma>=3.2.2 (from pymoo>=0.6.0->morl-baselines==1.1.0)\n","  Downloading cma-4.2.0-py3-none-any.whl.metadata (7.7 kB)\n","Collecting alive-progress (from pymoo>=0.6.0->morl-baselines==1.1.0)\n","  Downloading alive_progress-3.2.0-py3-none-any.whl.metadata (70 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from pymoo>=0.6.0->morl-baselines==1.1.0) (0.3.7)\n","Collecting Deprecated (from pymoo>=0.6.0->morl-baselines==1.1.0)\n","  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->morl-baselines==1.1.0) (3.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->morl-baselines==1.1.0) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->morl-baselines==1.1.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->morl-baselines==1.1.0) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.12.0->morl-baselines==1.1.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.12.0->morl-baselines==1.1.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.12.0->morl-baselines==1.1.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.12.0->morl-baselines==1.1.0)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.12.0->morl-baselines==1.1.0)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.12.0->morl-baselines==1.1.0)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.12.0->morl-baselines==1.1.0)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.12.0->morl-baselines==1.1.0)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.12.0->morl-baselines==1.1.0)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->morl-baselines==1.1.0) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->morl-baselines==1.1.0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->morl-baselines==1.1.0) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.12.0->morl-baselines==1.1.0)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->morl-baselines==1.1.0) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->morl-baselines==1.1.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.12.0->morl-baselines==1.1.0) (1.3.0)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.14.0->morl-baselines==1.1.0) (8.2.1)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.14.0->morl-baselines==1.1.0) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.14.0->morl-baselines==1.1.0) (3.1.44)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.14.0->morl-baselines==1.1.0) (4.3.8)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.14.0->morl-baselines==1.1.0) (5.29.5)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.14.0->morl-baselines==1.1.0) (5.9.5)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.14.0->morl-baselines==1.1.0) (2.11.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb>=0.14.0->morl-baselines==1.1.0) (6.0.2)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.14.0->morl-baselines==1.1.0) (2.32.3)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.14.0->morl-baselines==1.1.0) (2.29.1)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb>=0.14.0->morl-baselines==1.1.0) (1.3.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb>=0.14.0->morl-baselines==1.1.0) (75.2.0)\n","Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from cvxpy->morl-baselines==1.1.0) (1.0.4)\n","Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy->morl-baselines==1.1.0) (0.11.0)\n","Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/dist-packages (from cvxpy->morl-baselines==1.1.0) (3.2.7.post2)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->morl-baselines==1.1.0) (3.1.0)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio->morl-baselines==1.1.0) (11.2.1)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy->morl-baselines==1.1.0) (4.4.2)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.11/dist-packages (from moviepy->morl-baselines==1.1.0) (4.67.1)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy->morl-baselines==1.1.0) (0.1.12)\n","Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy->morl-baselines==1.1.0) (0.6.0)\n","Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn->morl-baselines==1.1.0) (2.2.2)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.11/dist-packages (from clarabel>=0.5.0->cvxpy->morl-baselines==1.1.0) (1.17.1)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.14.0->morl-baselines==1.1.0) (1.17.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.14.0->morl-baselines==1.1.0) (4.0.12)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines==1.1.0) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines==1.1.0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines==1.1.0) (4.58.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines==1.1.0) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines==1.1.0) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines==1.1.0) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines==1.1.0) (2.9.0.post0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from osqp>=0.6.2->cvxpy->morl-baselines==1.1.0) (1.5.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn->morl-baselines==1.1.0) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn->morl-baselines==1.1.0) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.14.0->morl-baselines==1.1.0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.14.0->morl-baselines==1.1.0) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.14.0->morl-baselines==1.1.0) (0.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb>=0.14.0->morl-baselines==1.1.0) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb>=0.14.0->morl-baselines==1.1.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb>=0.14.0->morl-baselines==1.1.0) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb>=0.14.0->morl-baselines==1.1.0) (2025.4.26)\n","Collecting about-time==4.2.1 (from alive-progress->pymoo>=0.6.0->morl-baselines==1.1.0)\n","  Downloading about_time-4.2.1-py3-none-any.whl.metadata (13 kB)\n","Collecting grapheme==0.6.0 (from alive-progress->pymoo>=0.6.0->morl-baselines==1.1.0)\n","  Downloading grapheme-0.6.0.tar.gz (207 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->pymoo>=0.6.0->morl-baselines==1.1.0) (1.17.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.12.0->morl-baselines==1.1.0) (3.0.2)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.14.0->morl-baselines==1.1.0) (5.0.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi->clarabel>=0.5.0->cvxpy->morl-baselines==1.1.0) (2.22)\n","Downloading pymoo-0.6.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cma-4.2.0-py3-none-any.whl (288 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alive_progress-3.2.0-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading about_time-4.2.1-py3-none-any.whl (13 kB)\n","Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n","Building wheels for collected packages: morl-baselines, fire, grapheme\n","  Building wheel for morl-baselines (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for morl-baselines: filename=morl_baselines-1.1.0-py3-none-any.whl size=129966 sha256=5fb0f6374600897f3faf2d995e0a482ab15ef736404a9eff265d466e61b2139c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-e8rlrw7m/wheels/b0/cd/e2/da3228d6d2ffb2f6c95ad330c10a9c17b7c8d89014cf9cf21a\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=18148d7ad2dbc677da91050fe2eb663e797c924c5823db33302d267f7eb592d0\n","  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n","  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for grapheme: filename=grapheme-0.6.0-py3-none-any.whl size=210082 sha256=4365b5dc987bcf03da7bfd0ab537cc7eca14a68be895719bcd34ee3e84b810b9\n","  Stored in directory: /root/.cache/pip/wheels/ee/3b/0b/1b865800e916d671a24028d884698674138632a83fdfad4926\n","Successfully built morl-baselines fire grapheme\n","Installing collected packages: grapheme, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, Deprecated, cma, about-time, nvidia-cusparse-cu12, nvidia-cudnn-cu12, alive-progress, pymoo, nvidia-cusolver-cu12, morl-baselines\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed Deprecated-1.2.18 about-time-4.2.1 alive-progress-3.2.0 cma-4.2.0 fire-0.7.0 grapheme-0.6.0 morl-baselines-1.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pymoo-0.6.1.5\n"]}],"source":["!pip install mo-gymnasium\n","!pip install git+https://github.com/LucasAlegre/morl-baselines.git"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16603,"status":"ok","timestamp":1749366397142,"user":{"displayName":"Liam Mertens","userId":"17654526473594897979"},"user_tz":-120},"id":"O17xAlg0STkZ","outputId":"77b5eb9b-b9ba-4e49-88c1-0268d05c3d2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":8715,"status":"ok","timestamp":1749381855722,"user":{"displayName":"Liam Mertens","userId":"17654526473594897979"},"user_tz":-120},"id":"PXKlO1PuRTEG"},"outputs":[],"source":["import gymnasium as gym\n","import mo_gymnasium as mo_gym\n","import numpy as np\n","import sys\n","sys.path.append('/content/drive/MyDrive')\n","from mo_awr.MO_AWR import MO_AWR"]},{"cell_type":"markdown","metadata":{"id":"gWhol4AjbaDS"},"source":["# Deep Sea Treasure\n","\n","A standard deterministic MOMDP which we can use to test the algorithm initially."]},{"cell_type":"code","execution_count":14,"metadata":{"collapsed":true,"executionInfo":{"elapsed":2676,"status":"ok","timestamp":1749379160083,"user":{"displayName":"Liam Mertens","userId":"17654526473594897979"},"user_tz":-120},"id":"-zMHHo5c7LLJ"},"outputs":[],"source":["env = mo_gym.make(\"deep-sea-treasure-v0\")\n","agent = MO_AWR(env, np.array([1,1,1]), log=False, td_lambda=.9, use_popf=False, max_buffer_size=128, value_lr=1e-3, policy_lr=1e-3, popf_lr=5e-4, batch_size=4, beta=.2, alpha=0.01, use_is_weighting=False)\n","\n","#agent.train(1000000, 20, num_value_steps=500, num_policy_steps=500, num_popf_steps=500, num_pf_points=10, num_expl_episodes=10, log_every=2, prune_pf_every=5, pf_prune_threshold=np.array([.1,1]))"]},{"cell_type":"code","source":["from morl_baselines.multi_policy.pareto_q_learning.pql import PQL\n","pql_agent = PQL(env, np.array([1,1]), gamma=1)\n","pql_agent.train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"id":"hqs0RIfT4gQg","executionInfo":{"status":"error","timestamp":1749377221414,"user_tz":-120,"elapsed":9544,"user":{"displayName":"Liam Mertens","userId":"17654526473594897979"}},"outputId":"ee927929-f2af-4d31-e18c-8a9e3b675dc4"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'env' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-0edc1cea088b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmorl_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpareto_q_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPQL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpql_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPQL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpql_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"PVtb6yKgQ1PN"},"source":["# Random MOMDP\n","\n","In order to test MO-AWR on a random environment, we can use the code at https://github.com/rradules/POP-following/blob/main/envs/randommomdp.py to generate an MOMDP where we can decide randomness and the problem size ourselves.\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YFFyXQ0hYTqB","outputId":"208df445-ece1-4551-af4d-4b73181a557a","collapsed":true,"executionInfo":{"status":"ok","timestamp":1749381859696,"user_tz":-120,"elapsed":40,"user":{"displayName":"Liam Mertens","userId":"17654526473594897979"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n","  gym.logger.warn(\n","/usr/local/lib/python3.11/dist-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n","  gym.logger.warn(\n"]}],"source":["env = mo_gym.make(\"random_momdp-v0\", nstates=5, nobjectives=2, nactions=3, nsuccessor=2, seed=12)\n","\n","from gymnasium.wrappers import FlattenObservation\n","#env = FlattenObservation(env)\n","\n","#agent = MO_AWR(env, log=False, td_lambda=.9, use_popf=True, max_buffer_size=100, value_lr=1e-3, policy_lr=5e-4, popf_lr=8e-4, batch_size=32, min_exploration_stdev=4, alpha=0.05, beta=.1, use_is_weighting=False)\n","#agent.train(1000000, 25, num_value_steps=100, num_policy_steps=100, num_popf_steps=100, num_pf_points=10, num_expl_episodes=10, log_every=3, prune_pf_every=10, num_eval_iter=15, pf_prune_threshold=np.array([1,1]))"]},{"cell_type":"markdown","metadata":{"id":"GOMR02WtAKU9"},"source":["# Water Reservoir (discrete)\n","A discrete version of a dam MOMDP (https://github.com/Farama-Foundation/MO-Gymnasium/blob/main/mo_gymnasium/envs/water_reservoir/dam_env.py).\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"zb-Kn0pFxxpj","outputId":"7611edff-e105-4d51-ef52-cfca422f47e4","executionInfo":{"status":"ok","timestamp":1749377250414,"user_tz":-120,"elapsed":40,"user":{"displayName":"Liam Mertens","userId":"17654526473594897979"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n","  gym.logger.warn(\n","/usr/local/lib/python3.11/dist-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n","  gym.logger.warn(\n"]}],"source":["env = mo_gym.make(\"dam_env-v0\", s_0=np.int64([5]), seed=0, penalize=False)\n","#agent = MO_AWR(env, np.array([0.1,0.1,0.1]), log=False, td_lambda=.9, use_popf=True, max_buffer_size=1024, value_lr=1e-3, policy_lr=5e-4, popf_lr=5e-4, batch_size=32, beta=.3, alpha=.01, use_is_weighting=False)\n","#agent.train(1000000, 250, num_value_steps=500, num_policy_steps=500, num_popf_steps=500, num_pf_points=7, num_expl_episodes=25, log_every=3, prune_pf_every=10, num_eval_iter=10, pf_prune_threshold=np.array([5,5]))"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"collapsed":true,"executionInfo":{"elapsed":428840,"status":"error","timestamp":1749382291226,"user":{"displayName":"Liam Mertens","userId":"17654526473594897979"},"user_tz":-120},"id":"vk1uWZDsBT9z","outputId":"1ebc5641-6d94-4879-8245-eabc316de661"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliam-mertens02\u001b[0m (\u001b[33mvub-ai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250608_112423-d6mequxh</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/vub-ai/MORL-Baselines/runs/d6mequxh' target=\"_blank\">random_momdp-v0__Pareto Q-Learning__None__1749381862</a></strong> to <a href='https://wandb.ai/vub-ai/MORL-Baselines' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/vub-ai/MORL-Baselines' target=\"_blank\">https://wandb.ai/vub-ai/MORL-Baselines</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/vub-ai/MORL-Baselines/runs/d6mequxh' target=\"_blank\">https://wandb.ai/vub-ai/MORL-Baselines/runs/d6mequxh</a>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-00b9addc4d51>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmorl_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpareto_q_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPQL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpql_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPQL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpql_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_eval_weights_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/morl_baselines/multi_policy/pareto_q_learning/pql.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, total_timesteps, eval_env, ref_point, known_pareto_front, num_eval_weights_for_eval, log_every, action_eval)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_dominated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_non_dominated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_reward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_reward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/morl_baselines/multi_policy/pareto_q_learning/pql.py\u001b[0m in \u001b[0;36mcalc_non_dominated\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m    195\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_q_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mnon_dominated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_non_dominated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnon_dominated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/morl_baselines/common/pareto.py\u001b[0m in \u001b[0;36mget_non_dominated\u001b[0;34m(candidates)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# since points are sorted by coordinate sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# i cannot dominate any points in 1,...,i-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mnon_dominated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnon_dominated\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Grab only the non-dominated vectors using the generated bitmask.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36many\u001b[0;34m(a, axis, out, keepdims, where)\u001b[0m\n\u001b[1;32m   2410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2411\u001b[0m     \"\"\"\n\u001b[0;32m-> 2412\u001b[0;31m     return _wrapreduction(a, np.logical_or, 'any', axis, None, out,\n\u001b[0m\u001b[1;32m   2413\u001b[0m                           keepdims=keepdims, where=where)\n\u001b[1;32m   2414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from morl_baselines.multi_policy.pareto_q_learning.pql import PQL\n","pql_agent = PQL(env, np.array([0, 0]), gamma=1)\n","pql_agent.train(500000,env, np.array([0,-30]), num_eval_weights_for_eval=5)"]},{"cell_type":"code","source":["pql_agent.get_local_pcs(0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"C_wADqoH_aVW","executionInfo":{"status":"ok","timestamp":1749380476937,"user_tz":-120,"elapsed":110,"user":{"displayName":"Liam Mertens","userId":"17654526473594897979"}},"outputId":"46cc791d-7f58-40e3-9b38-3148413e844c"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{(0.7025694688406048, -1.0),\n"," (0.7025701427143949, -2.0),\n"," (8.199999809265137, -3.0),\n"," (11.5, -5.0),\n"," (14.0, -7.0),\n"," (15.100000381469727, -8.0),\n"," (16.100000381469727, -9.0),\n"," (16.10019487993821, -11.0),\n"," (16.100194933995596, -12.0),\n"," (19.600000381469727, -13.0),\n"," (20.299999237060547, -14.0),\n"," (20.30019373552903, -16.0),\n"," (22.399999618530273, -17.0),\n"," (23.700000762939453, -19.0),\n"," (24.18333407810756, -21.0),\n"," (24.207500743865968, -22.0),\n"," (24.690834059034074, -23.0),\n"," (24.72801354481624, -24.0),\n"," (25.211346859984346, -25.0),\n"," (25.2766621728449, -26.0),\n"," (25.759995488013008, -27.0),\n"," (25.85666215104663, -28.0),\n"," (26.339995466214738, -29.0),\n"," (26.471813643078768, -30.0),\n"," (26.955146958246875, -31.0),\n"," (27.296593416699398, -32.0),\n"," (27.62599711888435, -33.0),\n"," (28.219320654747605, -34.0),\n"," (28.592663749220566, -35.0),\n"," (29.185987285083822, -36.0),\n"," (29.237839134986913, -37.0),\n"," (29.765987263285552, -38.0),\n"," (29.936588119974683, -39.0),\n"," (30.375982042854783, -40.0),\n"," (30.85931535802289, -41.0),\n"," (31.342648673191, -42.0),\n"," (31.825981988359107, -43.0),\n"," (31.877833838262198, -44.0),\n"," (31.914609199198907, -45.0),\n"," (32.28734560738321, -46.0),\n"," (32.75943861289625, -47.0),\n"," (32.81129046279934, -48.0),\n"," (32.84806582373605, -49.0),\n"," (33.14427556997193, -50.0),\n"," (33.196127419875026, -51.0),\n"," (33.59538666412883, -52.0),\n"," (34.05675028315294, -53.0),\n"," (34.10860213305603, -54.0),\n"," (34.46275026789415, -55.0),\n"," (34.514602117797246, -56.0),\n"," (34.86078946862083, -57.0),\n"," (34.912641318523924, -58.0),\n"," (34.94941667946063, -59.0),\n"," (35.24380832215027, -60.0),\n"," (35.63419292286297, -61.0),\n"," (35.68604477276607, -62.0),\n"," (35.722820133702776, -63.0),\n"," (35.74464808987165, -64.0),\n"," (35.76170691275994, -65.0),\n"," (35.776947152427404, -66.0),\n"," (35.792937680816955, -67.0),\n"," (35.80817792048442, -68.0),\n"," (35.81680887254099, -69.0),\n"," (35.821875625772584, -70.0),\n"," (35.830506577829155, -71.0),\n"," (35.83070107629764, -73.0),\n"," (35.830701130355024, -74.0),\n"," (35.83089562882351, -75.0),\n"," (35.83089573696834, -76.0),\n"," (35.831090235436825, -77.0),\n"," (35.83109039769917, -78.0),\n"," (35.83128489616766, -79.0),\n"," (35.83128511257763, -80.0),\n"," (35.83147961104612, -81.0),\n"," (35.83147988163386, -82.0),\n"," (35.83167438010235, -83.0),\n"," (35.831674704898006, -84.0),\n"," (35.831869203366495, -85.0),\n"," (35.83186958240026, -86.0),\n"," (35.83206408086875, -87.0),\n"," (35.83206451417082, -88.0),\n"," (35.832259012639305, -89.0),\n"," (35.83225950023992, -90.0),\n"," (35.83245399870841, -91.0),\n"," (35.832454540637826, -92.0),\n"," (35.832649039106315, -93.0),\n"," (35.832649635394816, -94.0),\n"," (35.832844133863304, -95.0),\n"," (35.8328447845412, -96.0),\n"," (35.83303928300969, -97.0),\n"," (35.833039988107316, -98.0),\n"," (35.833234486575805, -99.0),\n"," (35.833235246123515, -100.0),\n"," (35.833429744592, -101.0),\n"," (35.833430558620186, -102.0),\n"," (35.833625057088675, -103.0),\n"," (35.83362592562774, -104.0),\n"," (35.83382042409623, -105.0),\n"," (35.833821347176624, -106.0),\n"," (35.83401584564511, -107.0),\n"," (35.83401682329728, -108.0),\n"," (35.83421132176577, -109.0),\n"," (35.83421235402021, -110.0),\n"," (35.8344068524887, -111.0),\n"," (35.834407939375915, -112.0),\n"," (35.8346024378444, -113.0),\n"," (35.834603579394944, -114.0),\n"," (35.83479807786343, -115.0),\n"," (35.83479927410785, -116.0),\n"," (35.83499377257634, -117.0),\n"," (35.83499514068943, -118.0),\n"," (35.83518952201372, -119.0),\n"," (35.835191164208915, -120.0),\n"," (35.83538532620619, -121.0),\n"," (35.83538724263695, -122.0),\n"," (35.83558118518439, -123.0),\n"," (35.8355833760043, -124.0),\n"," (35.83577709897898, -125.0),\n"," (35.83577956434177, -126.0),\n"," (35.83597306762066, -127.0),\n"," (35.83597580768017, -128.0),\n"," (35.83616909114014, -129.0),\n"," (35.836172106050356, -130.0),\n"," (35.836365169568175, -131.0),\n"," (35.8363684594832, -132.0),\n"," (35.83656130293553, -133.0),\n"," (35.836564868009596, -134.0),\n"," (35.836757491273, -135.0),\n"," (35.836761331660476, -136.0),\n"," (35.8369537346114, -137.0),\n"," (35.83695785046679, -138.0),\n"," (35.83715003298158, -139.0),\n"," (35.83715442445951, -140.0),\n"," (35.837346386414424, -141.0),\n"," (35.83735105366965, -142.0),\n"," (35.83754279494082, -143.0),\n"," (35.83754773812823, -144.0),\n"," (35.837739258591704, -145.0),\n"," (35.83774447786631, -146.0),\n"," (35.837935777398016, -147.0),\n"," (35.83794127291497, -148.0),\n"," (35.83813235139074, -149.0),\n"," (35.83813812330531, -150.0),\n"," (35.83832898060088, -151.0),\n"," (35.83833502906849, -152.0),\n"," (35.83852566505946, -153.0),\n"," (35.838531990235644, -154.0),\n"," (35.83872240479754, -155.0),\n"," (35.83872900683797, -156.0),\n"," (35.838919199846195, -157.0),\n"," (35.83892607890669, -158.0),\n"," (35.83911605023654, -159.0),\n"," (35.83912320647303, -160.0),\n"," (35.839312955999716, -161.0),\n"," (35.839320389568265, -162.0),\n"," (35.83950991716687, -163.0),\n"," (35.83951762822368, -164.0),\n"," (35.8397069337692, -165.0),\n"," (35.839714922470606, -166.0),\n"," (35.839904005837916, -167.0),\n"," (35.83991227234038, -168.0),\n"," (35.84010113340426, -169.0),\n"," (35.84010967786437, -170.0),\n"," (35.84029831649949, -171.0),\n"," (35.840307139073985, -172.0),\n"," (35.84049555515491, -173.0),\n"," (35.84050465600065, -174.0),\n"," (35.84069284940183, -175.0),\n"," (35.8407022286758, -176.0),\n"," (35.840890199271605, -177.0),\n"," (35.840899857130935, -178.0),\n"," (35.8410876047956, -179.0),\n"," (35.84109754139755, -180.0),\n"," (35.84128506600521, -181.0),\n"," (35.84129528150718, -182.0),\n"," (35.841482582931874, -183.0),\n"," (35.84149307749138, -184.0),\n"," (35.84168015560703, -185.0),\n"," (35.841690929381734, -186.0),\n"," (35.84187778406216, -187.0),\n"," (35.84188883720986, -188.0),\n"," (35.84207546832878, -189.0),\n"," (35.84208680100739, -190.0),\n"," (35.84227320843841, -191.0),\n"," (35.842284820806, -192.0),\n"," (35.842471004422606, -193.0),\n"," (35.84248289663738, -194.0),\n"," (35.84266885631296, -195.0),\n"," (35.84268102853324, -196.0),\n"," (35.842866764141085, -197.0),\n"," (35.84287921652534, -198.0),\n"," (35.84306472793862, -199.0),\n"," (35.84307746064544, -200.0),\n"," (35.84326274773723, -201.0),\n"," (35.84327576092535, -202.0),\n"," (35.843460823568606, -203.0),\n"," (35.843474117396894, -204.0),\n"," (35.84365895546447, -205.0),\n"," (35.84367253009193, -206.0),\n"," (35.84385714345657, -207.0),\n"," (35.84387099904234, -208.0),\n"," (35.84405538757667, -209.0),\n"," (35.84406952428002, -210.0),\n"," (35.84425368785658, -211.0),\n"," (35.844268105836925, -212.0),\n"," (35.84445204432812, -213.0),\n"," (35.84446674374501, -214.0),\n"," (35.84465045702316, -215.0),\n"," (35.84466543803626, -216.0),\n"," (35.844848925973565, -217.0),\n"," (35.8448641887427, -218.0),\n"," (35.84504745121125, -219.0),\n"," (35.84506299589637, -220.0),\n"," (35.84524603276815, -221.0),\n"," (35.845261859529344, -222.0),\n"," (35.84544467067624, -223.0),\n"," (35.84546077967373, -224.0),\n"," (35.845643364967486, -225.0),\n"," (35.84565975636164, -226.0),\n"," (35.845842115673925, -227.0),\n"," (35.84585878962524, -228.0),\n"," (35.8460409228276, -229.0),\n"," (35.846057879496705, -230.0),\n"," (35.84623978646057, -231.0),\n"," (35.84625702600825, -232.0),\n"," (35.84643870660496, -233.0),\n"," (35.846456229192114, -234.0),\n"," (35.84663768329287, -235.0),\n"," (35.846655489080554, -236.0),\n"," (35.84683671655647, -237.0),\n"," (35.84685480570586, -238.0),\n"," (35.84703580642793, -239.0),\n"," (35.847054179100354, -240.0),\n"," (35.84723495293948, -241.0),\n"," (35.84725360929639, -242.0),\n"," (35.84743415612334, -243.0),\n"," (35.84745309632633, -244.0),\n"," (35.84763341601178, -245.0),\n"," (35.84765264022259, -246.0),\n"," (35.84783273263709, -247.0),\n"," (35.8478522410176, -248.0),\n"," (35.84803210603158, -249.0),\n"," (35.848051898743805, -250.0),\n"," (35.848231536227615, -251.0),\n"," (35.848251613433696, -252.0),\n"," (35.84843102325756, -253.0),\n"," (35.848451385119795, -254.0),\n"," (35.84863056715382, -255.0),\n"," (35.848651213834636, -256.0),\n"," (35.848830167948826, -257.0),\n"," (35.84885109961078, -258.0),\n"," (35.84902982567503, -259.0),\n"," (35.849051042480845, -260.0),\n"," (35.849229540364924, -261.0),\n"," (35.84925104247744, -262.0),\n"," (35.84942931205102, -263.0),\n"," (35.84945109963322, -264.0),\n"," (35.84962914076586, -265.0),\n"," (35.84965121398087, -266.0),\n"," (35.84982902654201, -267.0),\n"," (35.849851385553094, -268.0),\n"," (35.85002896941207, -269.0),\n"," (35.85004686167375, -270.0),\n"," (35.85022896940867, -271.0),\n"," (35.85024321143121, -272.0),\n"," (35.85042902656445, -273.0),\n"," (35.850438523927885, -274.0),\n"," (35.8506291409121, -275.0),\n"," (35.850634814015834, -276.0),\n"," (35.85082931248432, -277.0),\n"," (35.850853102929676, -280.0),\n"," (35.85102954131386, -281.0),\n"," (35.85105361853783, -282.0),\n"," (35.851229827433485, -283.0),\n"," (35.85125419160032, -284.0),\n"," (35.85143017087598, -285.0),\n"," (35.85145482215007, -286.0),\n"," (35.85163057167417, -287.0),\n"," (35.85165551022004, -288.0),\n"," (35.851831029860904, -289.0),\n"," (35.85185625584324, -290.0),\n"," (35.85203154546906, -291.0),\n"," (35.85205705905267, -292.0),\n"," (35.852232118531546, -293.0),\n"," (35.85225791988139, -294.0),\n"," (35.852432749081295, -295.0),\n"," (35.852458838362466, -296.0),\n"," (35.85263343715127, -297.0),\n"," (35.85265981452901, -298.0),\n"," (35.852834182774465, -299.0),\n"," (35.85285529064967, -300.0),\n"," (35.853034985983896, -301.0),\n"," (35.85305164040713, -302.0),\n"," (35.853235846812616, -303.0),\n"," (35.8532469529038, -304.0),\n"," (35.853436765293694, -305.0),\n"," (35.85344324299175, -306.0),\n"," (35.85363774146024, -307.0)}"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["from morl_baselines.common.performance_indicators import hypervolume\n","print(hypervolume(np.array([-150,-50]), np.array(list(pql_agent.get_local_pcs(0)))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZlKYPQt_nOa","executionInfo":{"status":"ok","timestamp":1749377987307,"user_tz":-120,"elapsed":446,"user":{"displayName":"Liam Mertens","userId":"17654526473594897979"}},"outputId":"dedd14ca-ef10-4009-a3ea-2b888fe3d8c2"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["6416.022941916653\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def eval_pcn(n, n_points):\n","  evals = np.zeros((n_points,2))\n","  for i in range(n):\n","    e, p, _ = pcn_agent.evaluate(env, np.array([100,100]), n_points)\n","    evals += np.array(e)\n","  evals /= n\n","  return evals, p\n","def plot_eval(evals, pf, filename):\n","  x2, y2 = zip(*pf)\n","  x3, y3 = zip(*evals)\n","  plt.figure(figsize=(8, 6))\n","  plt.scatter(x2, y2, color='red', label='Pareto front')\n","  plt.scatter(x3, y3, color='green', label='evaluated pf')\n","\n","  # Add labels and legend\n","  plt.xlabel('Obj 1')\n","  plt.ylabel('Obj 2')\n","  plt.legend()\n","  plt.title('Comparison of PF and evaluations')\n","  plt.savefig(filename)\n","  plt.show()\n","def make_csv(evals, pf, filename):\n","  comb = np.hstack((pf, evals))\n","  np.savetxt(filename, comb, delimiter=\",\", fmt='%.2f')"],"metadata":{"id":"UoNEO8qppDUm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evals, pf = eval_pcn(20, 7)\n","plot_eval(evals, pf, \"pcn_eval.png\")\n","make_csv(evals, pf, \"pcn_eval.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"id":"upPcqs7Xp0g2","executionInfo":{"status":"ok","timestamp":1749323119291,"user_tz":-120,"elapsed":7683,"user":{"displayName":"Liam Mertens","userId":"17654526473594897979"}},"outputId":"591aa4d0-7177-498e-95f4-21de979675a3"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 800x600 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAroAAAIjCAYAAADslLiSAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUWVJREFUeJzt3X1cVGX+//H3gDAoCt5xpyCIpKKWa7q5qKRWKpWpmVlpKW25lbapaYVrZVlq3muWZjfeZLmVxnZnWZq1S6Zt+ZNuDC3NWwS1VPCmQOH6/cGXWUdABgVm5vh6Ph7z0LnOdc58Zs4ceXu4znVsxhgjAAAAwGJ83F0AAAAAUBUIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugC8ls1m0xNPPOHuMi7YsmXL1LJlS/n5+alu3bruLqda7Nq1SzabTUuWLHF3KWVy5/fr888/l81m0+eff+6W1wesgqALeLEdO3bonnvuUWxsrAICAhQUFKTOnTtr7ty5+v33391dHlywdetWJScnq1mzZnrppZf04osvltn3iSeekM1mczxq1aqlVq1a6dFHH1Vubq6j35IlS5z6nflISUmpjrcFF82fP9+jwz7g7Wq4uwAA52fVqlW6+eabZbfbNWTIELVp00b5+fn64osv9NBDD2nLli3nDE1W8Pvvv6tGDe/+Z+zzzz9XYWGh5s6dq7i4OJfWWbBggWrXrq3jx4/rk08+0aRJk7Ru3TqtX79eNpvN0W/ixIlq2rSp07pt2rSp1PpxYebPn6+GDRsqOTnZqf3KK6/U77//Ln9/f/cUBliEd/+EAC5SO3fu1K233qro6GitW7dOERERjmUjRozQ9u3btWrVKjdWWHUKCwuVn5+vgIAABQQEuLucC3bw4EFJqtCQhQEDBqhhw4aSpHvvvVc33XSTUlNTtXHjRiUkJDj6XXvtterQoUOl1ovq4ePjY4nvN+BuDF0AvNC0adN0/PhxvfLKK04ht1hcXJxGjhzpeH769Gk99dRTatasmex2u2JiYvSPf/xDeXl5TuvFxMSod+/e+vzzz9WhQwfVrFlTl156qWOcYGpqqi699FIFBASoffv22rx5s9P6ycnJql27tn755Rf16tVLgYGBatSokSZOnChjjFPfGTNmqFOnTmrQoIFq1qyp9u3ba+XKlSXei81m0/3336/XX39drVu3lt1u1+rVqx3LzhxDeezYMY0aNUoxMTGy2+0KDQ1Vjx499P/+3/9z2uaKFSvUvn171axZUw0bNtTtt9+uzMzMUt9LZmam+vXrp9q1ayskJERjx45VQUFBGXvG2fz58x01N2rUSCNGjNDRo0edPu8JEyZIkkJCQs57TOhVV10lqeg/QBcqLS1NN998s5o0aSK73a6oqCiNHj26xFCYinw+R48eVXJysoKDg1W3bl0NHTrU6XMoz9GjRzVq1ChFRUXJbrcrLi5OU6dOVWFhoSTp1KlTql+/vu68884S6+bm5iogIEBjx46VJOXn5+vxxx9X+/btFRwcrMDAQCUmJuqzzz4rt47k5GTFxMSUaC8eUnKmxYsX66qrrlJoaKjsdrtatWqlBQsWOPWJiYnRli1b9O9//9sxtKRbt26Syh6jW9nf3TfeeEPt27dXnTp1FBQUpEsvvVRz584t97MAvAVBF/BC77//vmJjY9WpUyeX+t999916/PHHdfnll2v27Nnq2rWrpkyZoltvvbVE3+3bt2vQoEG64YYbNGXKFB05ckQ33HCDXn/9dY0ePVq33367nnzySe3YsUMDBw50hI1iBQUFSkpKUlhYmKZNm6b27dtrwoQJjkBXbO7cuWrXrp0mTpyoyZMnq0aNGrr55ptLPRO9bt06jR49Wrfccovmzp1batiQis5uLliwQDfddJPmz5+vsWPHqmbNmsrIyHD0WbJkiQYOHChfX19NmTJFw4YNU2pqqrp06VIifBUUFKhXr15q0KCBZsyYoa5du2rmzJkuDQl54oknNGLECDVq1EgzZ87UTTfdpIULF6pnz546deqUJGnOnDm68cYbJRUNR1i2bJn69+9f7rbPtmPHDklSgwYNnNpzcnL066+/Oj3Ks2LFCp08eVL33Xef5s2bp169emnevHkaMmRIib6ufD7GGPXt21fLli3T7bffrqefflr79u3T0KFDXXpvJ0+eVNeuXfXaa69pyJAhevbZZ9W5c2eNGzdODz74oCTJz89PN954o9555x3l5+c7rf/OO+8oLy/P8V3Pzc3Vyy+/rG7dumnq1Kl64okndOjQIfXq1Uvp6eku1eSKBQsWKDo6Wv/4xz80c+ZMRUVFafjw4Xr++ecdfebMmaPIyEi1bNlSy5Yt07JlyzR+/Pgyt1nZ3901a9botttuU7169TR16lQ988wz6tatm9avX19pnwPgdgaAV8nJyTGSTN++fV3qn56ebiSZu+++26l97NixRpJZt26doy06OtpIMl9++aWj7eOPPzaSTM2aNc3u3bsd7QsXLjSSzGeffeZoGzp0qJFk/v73vzvaCgsLzfXXX2/8/f3NoUOHHO0nT550qic/P9+0adPGXHXVVU7tkoyPj4/ZsmVLifcmyUyYMMHxPDg42IwYMaLMzyI/P9+EhoaaNm3amN9//93R/sEHHxhJ5vHHHy/xXiZOnOi0jXbt2pn27duX+RrGGHPw4EHj7+9vevbsaQoKChztzz33nJFkFi1a5GibMGGCkeT02ZSluO+2bdvMoUOHzM6dO83ChQuN3W43YWFh5sSJE8YYYxYvXmwklfooz9n7xRhjpkyZYmw2m9P+d/Xzeeedd4wkM23aNEfb6dOnTWJiopFkFi9efM56nnrqKRMYGGh++uknp/aUlBTj6+tr9uzZY4z53/f0/fffd+p33XXXmdjYWKfXzsvLc+pz5MgRExYWZv761786tZ/9/Ro6dKiJjo4uUWPxfjlTaZ9jr169nGoxxpjWrVubrl27luj72WefOR1fVfHdHTlypAkKCjKnT58u8fqAVXBGF/AyxVfX16lTx6X+H374oSQ5zn4VGzNmjCSVOIPaqlUrp3GeHTt2lFT06/EmTZqUaP/ll19KvOb999/v+Hvx0IP8/HytXbvW0V6zZk3H348cOaKcnBwlJiaWGGYgSV27dlWrVq3KeadF41y/+uor7d+/v9Tl33zzjQ4ePKjhw4c7jX+8/vrr1bJly1LPJt97771OzxMTE0t9z2dau3at8vPzNWrUKPn4/O+f2WHDhikoKOiCx0+3aNFCISEhatq0qe655x7FxcVp1apVqlWrllO/559/XmvWrHF6lOfM/XLixAn9+uuv6tSpk4wxJYaqSOV/Ph9++KFq1Kih++67z9Hm6+urv//97y691xUrVigxMVH16tVzOjN9zTXXqKCgQP/5z38kFX0/GzZsqDfffNOx7pEjR7RmzRrdcsstTq9dfIFXYWGhDh8+rNOnT6tDhw6lfvfO15mfY/GZ9a5du+qXX35RTk5OhbdXFd/dunXr6sSJEy59LwBvxcVogJcJCgqSVDQe1RW7d++Wj49PiSv6w8PDVbduXe3evdup/cwwK0nBwcGSpKioqFLbjxw54tTu4+Oj2NhYp7bmzZtLKpo7tdgHH3ygp59+Wunp6U5jhc8e6yipxMwBZZk2bZqGDh2qqKgotW/fXtddd52GDBniqKf4vbZo0aLEui1bttQXX3zh1BYQEKCQkBCntnr16pV4z2cr63X8/f0VGxtb4jOvqLfffltBQUHy8/NTZGSkmjVrVmq/K664osIXo+3Zs0ePP/643nvvvRLv8+yA5srns3v3bkVERKh27dpO/UrbB6X5+eef9d1335V4nWLFF/PVqFFDN910k5YvX668vDzZ7Xalpqbq1KlTTkFXkpYuXaqZM2dq69atjmEkkuvfM1esX79eEyZM0IYNG3Ty5EmnZTk5OY7jx1VV8d0dPny43nrrLV177bVq3LixevbsqYEDByopKalCtQGejKALeJmgoCA1atRIP/zwQ4XWKy1AlsbX17dC7easi8xckZaWpj59+ujKK6/U/PnzFRERIT8/Py1evFjLly8v0f/Ms2PnMnDgQCUmJupf//qXPvnkE02fPl1Tp05Vamqqrr322grXWdZ7drcrr7zSMetCZSooKFCPHj10+PBhPfLII2rZsqUCAwOVmZmp5OTkEuOxq+PzKSwsVI8ePfTwww+Xurz4P1GSdOutt2rhwoX66KOP1K9fP7311ltq2bKl2rZt6+jz2muvKTk5Wf369dNDDz2k0NBQx5jX4rHOZSnrGDr7Aq8dO3bo6quvVsuWLTVr1ixFRUXJ399fH374oWbPnl3ic6wKruyb0NBQpaen6+OPP9ZHH32kjz76SIsXL9aQIUO0dOnSKq8RqA4EXcAL9e7dWy+++KI2bNjgNMygNNHR0SosLNTPP/+s+Ph4R/uBAwd09OhRRUdHV2pthYWF+uWXX5wCyE8//SRJjovI3n77bQUEBOjjjz+W3W539Fu8ePEFv35ERISGDx+u4cOH6+DBg7r88ss1adIkXXvttY73um3bNsdMBcW2bdtWaZ/Fma9z5tnt/Px87dy5U9dcc02lvE5l+/777/XTTz9p6dKlThefXcivtqOjo/Xpp5/q+PHjTmd1t23b5tL6zZo10/Hjx136zK688kpFRETozTffVJcuXbRu3boSF3etXLlSsbGxSk1NdQquZ18sWZp69eqVOlvE2Wfo33//feXl5em9995z+g1JaTM7uPof0Kr67vr7++uGG27QDTfcoMLCQg0fPlwLFy7UY4895vK8zoAnY4wu4IUefvhhBQYG6u6779aBAwdKLN+xY4djiqDrrrtOUtEV3meaNWuWpKIxfpXtueeec/zdGKPnnntOfn5+uvrqqyUVnW2y2WxOZ8J27dqld95557xfs6CgoMSv1kNDQ9WoUSPH0IgOHTooNDRUL7zwgtNwiY8++kgZGRmV9llcc8018vf317PPPut0xvuVV15RTk5OlXzmlaH4LOCZNRtjLmi6qeuuu06nT592mlqroKBA8+bNc2n9gQMHasOGDfr4449LLDt69KhOnz7teO7j46MBAwbo/fff17Jly3T69OkSwxZKe49fffWVNmzYUG4tzZo1U05Ojr777jtHW1ZWlv71r3+V+xo5OTml/kcuMDDQpanWquK7+9tvvzk99/Hx0WWXXSZJJaYeBLwVZ3QBL9SsWTMtX75ct9xyi+Lj453ujPbll19qxYoVjjsttW3bVkOHDtWLL76oo0ePqmvXrvrvf/+rpUuXql+/furevXul1hYQEKDVq1dr6NCh6tixoz766COtWrVK//jHPxxjBq+//nrNmjVLSUlJGjRokA4ePKjnn39ecXFxTiGiIo4dO6bIyEgNGDBAbdu2Ve3atbV27Vp9/fXXmjlzpqSiaaimTp2qO++8U127dtVtt92mAwcOOKYsGz16dKV8BiEhIRo3bpyefPJJJSUlqU+fPtq2bZvmz5+vP//5z7r99tsr5XUqW8uWLdWsWTONHTtWmZmZCgoK0ttvv13umORzueGGG9S5c2elpKRo165datWqlVJTU12+IOuhhx7Se++9p969eys5OVnt27fXiRMn9P3332vlypXatWuX0zCOW265RfPmzdOECRN06aWXOv0WQyr6bUhqaqpuvPFGXX/99dq5c6deeOEFtWrVSsePHz9nLbfeeqseeeQR3XjjjXrggQd08uRJLViwQM2bN3e6kK1nz56OM6X33HOPjh8/rpdeekmhoaHKyspy2mb79u21YMECPf3004qLi1NoaGiJM7ZS1Xx37777bh0+fFhXXXWVIiMjtXv3bs2bN09/+tOfSnxugNdy34QPAC7UTz/9ZIYNG2ZiYmKMv7+/qVOnjuncubOZN2+e+eOPPxz9Tp06ZZ588knTtGlT4+fnZ6Kiosy4ceOc+hhTNL3Y9ddfX+J1JJWYtmvnzp1Gkpk+fbqjbejQoSYwMNDs2LHD9OzZ09SqVcuEhYWZCRMmOE2zZYwxr7zyirnkkkuM3W43LVu2NIsXLy51mqbSXvvMZcXTP+Xl5ZmHHnrItG3b1tSpU8cEBgaatm3bmvnz55dY78033zTt2rUzdrvd1K9f3wwePNjs27fPqU/xezlbaTWW5bnnnjMtW7Y0fn5+JiwszNx3333myJEjpW6vItOLlde3eHqxr7/+2qU6z/Tjjz+aa665xtSuXds0bNjQDBs2zHz77bclpgKryOfz22+/mTvuuMMEBQWZ4OBgc8cdd5jNmze7NL2YMcYcO3bMjBs3zsTFxRl/f3/TsGFD06lTJzNjxgyTn5/v1LewsNBERUUZSebpp58usa3CwkIzefJkEx0dbex2u2nXrp354IMPSp06TGdNL2aMMZ988olp06aN8ff3Ny1atDCvvfZaqe/5vffeM5dddpkJCAgwMTExZurUqWbRokVGktm5c6ejX3Z2trn++utNnTp1jCTHVGNnTy9WrDK/uytXrjQ9e/Y0oaGhxt/f3zRp0sTcc889Jisrq8S6gLeyGXMeV5IAQCmSk5O1cuXKcs+MAQBQHRijCwAAAEsi6AIAAMCSCLoAAACwJMboAgAAwJI4owsAAABLIugCAADAkrhhxFkKCwu1f/9+1alTx+VbMwIAAKD6GGN07NgxNWrUSD4+ZZ+3JeieZf/+/YqKinJ3GQAAACjH3r17FRkZWeZygu5Z6tSpI6nogwsKCnJzNQAAADhbbm6uoqKiHLmtLATdsxQPVwgKCiLoAgAAeLDyhplyMRoAAAAsiaALAAAASyLoAgAAwJIYowsAANzCGKPTp0+roKDA3aXAw/j6+qpGjRoXPNUrQRcAAFS7/Px8ZWVl6eTJk+4uBR6qVq1aioiIkL+//3lvg6ALAACqVWFhoXbu3ClfX181atRI/v7+3KQJDsYY5efn69ChQ9q5c6cuueSSc94U4lwIugAAoFrl5+ersLBQUVFRqlWrlrvLgQeqWbOm/Pz8tHv3buXn5ysgIOC8tsPFaAAAwC3O9ywdLg6V8f3gGwYAAABLIugCAADAkgi6AAAAF4ns7Gz16NFDgYGBqlu3rrvLqXIEXQAAABckJyfLZrPJZrPJ399fcXFxmjhxok6fPl2lr7tkyZJKC6WzZ89WVlaW0tPT9dNPP1XKNsvy+eefy2az6ejRo1X6OufCrAsAAMA7FRRIaWlSVpYUESElJkq+vlX6kklJSVq8eLHy8vL04YcfasSIEfLz89O4ceMqvK2CggLZbLZqvShvx44dat++vS655JIy+5w6dUp+fn7VVlNV4owuALiioED6/HPpn/8s+pM7OQHulZoqxcRI3btLgwYV/RkTU9Rehex2u8LDwxUdHa377rtP11xzjd577z1J0qxZs3TppZcqMDBQUVFRGj58uI4fP+5Yt/jM7HvvvadWrVrJbrdrz549ysvL09ixY9W4cWMFBgaqY8eO+vzzzyUVnRW98847lZOT4zib/MQTT0iSjhw5oiFDhqhevXqqVauWrr32Wv38889l1h4TE6O3335br776qmw2m5KTkyVJNptNCxYsUJ8+fRQYGKhJkyZJkhYsWKBmzZrJ399fLVq00LJly5y2Z7PZ9PLLL+vGG29UrVq1dMkllzg+i127dql79+6SpHr16jm9XrUyFvTcc8+Z6OhoY7fbzRVXXGG++uorl9fNyckxkkxOTk4VVgjAq7z9tjGRkcZI/3tERha1A6iw33//3fz444/m999/P78NvP22MTab8zEpFbXZbFV2bA4dOtT07dvXqa1Pnz7m8ssvN8YYM3v2bLNu3Tqzc+dO8+mnn5oWLVqY++67z9F38eLFxs/Pz3Tq1MmsX7/ebN261Zw4ccLcfffdplOnTuY///mP2b59u5k+fbqx2+3mp59+Mnl5eWbOnDkmKCjIZGVlmaysLHPs2DHHa8fHx5v//Oc/Jj093fTq1cvExcWZ/Pz8Uus/ePCgSUpKMgMHDjRZWVnm6NGjxhhjJJnQ0FCzaNEis2PHDrN7926Tmppq/Pz8zPPPP2+2bdtmZs6caXx9fc26desc25NkIiMjzfLly83PP/9sHnjgAVO7dm3z22+/mdOnT5u3337bSDLbtm1zej1Xnet74mpes1zQfeONN4y/v79ZtGiR2bJlixk2bJipW7euOXDggEvrE3QBOHHTD1TAyi4o6J4+XfI/nmcfm1FRRf0q2ZlBt7Cw0KxZs8bY7XYzduzYUvuvWLHCNGjQwPF88eLFRpJJT093tO3evdv4+vqazMxMp3WvvvpqM27cOMd6wcHBTst/+uknI8msX7/e0fbrr7+amjVrmrfeeqvM99C3b18zdOhQpzZJZtSoUU5tnTp1MsOGDXNqu/nmm811113ntN6jjz7qeH78+HEjyXz00UfGGGM+++wzI8kcOXKkzHrOpTKCruWGLsyaNUvDhg3TnXfeqVatWumFF15QrVq1tGjRIneXBsDbFBRII0cW/fg8W3HbqFEMYwCqU1qatG9f2cuNkfbuLepXBT744APVrl1bAQEBuvbaa3XLLbc4hhKsXbtWV199tRo3bqw6derojjvu0G+//aaTJ0861vf399dll13meP7999+roKBAzZs3V+3atR2Pf//739qxY0eZdWRkZKhGjRrq2LGjo61BgwZq0aKFMjIyKvy+OnToUGL7nTt3dmrr3LlziW2f+V4CAwMVFBSkgwcPVvj1q4qlLkbLz8/Xpk2bnAaE+/j46JprrtGGDRtKXScvL095eXmO57m5uVVeJwAvUZEfqN26VVtZwEUtK6ty+1VQ9+7dtWDBAvn7+6tRo0aqUaMoSu3atUu9e/fWfffdp0mTJql+/fr64osvdNdddyk/P99xq+OaNWvKZrM5tnf8+HH5+vpq06ZN8j3rQrratWtXyXsoTWBg4Hmtd/ZFazabTYWFhZVRUqWw1BndX3/9VQUFBQoLC3NqDwsLU3Z2dqnrTJkyRcHBwY5HVFRUdZQKwBu4+QcqgFJERFRuvwoKDAxUXFycmjRp4gi5krRp0yYVFhZq5syZ+stf/qLmzZtr//795W6vXbt2Kigo0MGDBxUXF+f0CA8Pl1R0FrjgrN8cxcfH6/Tp0/rqq68cbb/99pu2bdumVq1aXfD7jI+P1/r1653a1q9fX6Ft+/v7S1KJ2quTpYLu+Rg3bpxycnIcj71797q7JACews0/UAGUIjFRioyUzjgr6sRmk6KiivpVo7i4OJ06dUrz5s3TL7/8omXLlumFF14od73mzZtr8ODBGjJkiFJTU7Vz507997//1ZQpU7Rq1SpJRbMlHD9+XJ9++ql+/fVXnTx5Updccon69u2rYcOG6YsvvtC3336r22+/XY0bN1bfvn0v+P089NBDWrJkiRYsWKCff/5Zs2bNUmpqqsaOHevyNqKjo2Wz2fTBBx/o0KFDTjNQVBdLBd2GDRvK19dXBw4ccGo/cOCA439FZ7Pb7QoKCnJ6AIAkj/2BClzUfH2luXOL/n72sVn8fM6cKp9P92xt27bVrFmzNHXqVLVp00avv/66pkyZ4tK6ixcv1pAhQzRmzBi1aNFC/fr109dff60mTZpIkjp16qR7771Xt9xyi0JCQjRt2jTHeu3bt1fv3r2VkJAgY4w+/PDDSpkDt1+/fpo7d65mzJih1q1ba+HChVq8eLG6VWCYVuPGjfXkk08qJSVFYWFhuv/++y+4roqy/d9Vc5bRsWNHXXHFFZo3b54kqbCwUE2aNNH999+vlJSUctfPzc1VcHCwcnJyCL0AiubkHDCg6O9n/nNZ/AN15Uqpf//qrwvwYn/88Yd27typpk2bKiAg4Pw2kppadLHomePoo6KKQi7HpCWc63vial6z1MVokvTggw9q6NCh6tChg6644grNmTNHJ06c0J133unu0gB4o/79i8Ls2T9QIyP5gQq4U//+Ut++1X5nNHgXywXdW265RYcOHdLjjz+u7Oxs/elPf9Lq1atLXKAGAC7jByrgmXx9mfEE52S5oCtJ999/v1vGgQCwMH6gAoDXsdTFaAAAAEAxgi4AAAAsyZJDFwAAHqSggPHNANyCoAsAqDqlTQEVGVk0DyozVgCoYgxdcKOCwgJ9vutz/fP7f+rzXZ+roNB9t8gDgEpXPAfxmSFXkjIzi9pTU91TF4CLBmd03SQ1I1UjV4/Uvtz//QCIDIrU3KS56h/PWQ4AXq6goOhMbmn3JDKm6IYbo0YVTdvGMAYAVYQzum6QmpGqAW8NcAq5kpSZm6kBbw1QagZnOQB4ubS0kmdyz2SMtHdvUT8ADkuWLFHdunXdXUaZdu3aJZvNpvT09AvaTnZ2tnr06KHAwMAqfb8E3WpWUFigkatHyqjkWY7itlGrRzGMAYB3y8qq3H4AzltlhdPKNHv2bGVlZSk9PV0//fRTlb0OQxeqWdqetBJncs9kZLQ3d6/S9qSpW0y36isMACpTRETl9gNKUVBYoLQ9aco6lqWIOhFKbJIoXx+GwniDHTt2qH379rrkkkuq9HU4o1vNso65dvbC1X4A4JESE4tmV7DZSl9us0lRUUX9gPOQmpGqmLkx6r60uwalDlL3pd0VMzemSof/FRYWasqUKWratKlq1qyptm3bauXKlY5lkZGRWrBggdM6mzdvlo+Pj3bv3i1JmjVrli699FIFBgYqKipKw4cP1/Hjx8t8zeTkZPXr18+pbdSoUep2xp0aV69erS5duqhu3bpq0KCBevfurR07djiWN23aVJLUrl072Ww2p3VffvllxcfHKyAgQC1bttT8+fOdXuu///2v2rVrp4CAAHXo0EGbN28u93OKiYnRU089pdtuu02BgYFq3Lixnn/+eaflb7/9tl599VXZbDYlJyeXu83zRdCtZhF1XDt74Wo/APBIvr5FU4hJJcNu8fM5c7gQDefFXde6TJkyRa+++qpeeOEFbdmyRaNHj9btt9+uf//73/Lx8dFtt92m5cuXO63z+uuvq3PnzoqOjpYk+fj46Nlnn9WWLVu0dOlSrVu3Tg8//PAF1XXixAk9+OCD+uabb/Tpp5/Kx8dHN954owoLCyUVhVVJWrt2rbKyspT6fzOevP7663r88cc1adIkZWRkaPLkyXrssce0dOlSSdLx48fVu3dvtWrVSps2bdITTzyhsWPHulTT9OnT1bZtW23evFkpKSkaOXKk1qxZI0n6+uuvlZSUpIEDByorK0tzi/+tqAIMXahmiU0SFRkUqczczFLH6dpkU2RQpBKbcJYDgJfr319aubL0eXTnzGEeXZyX8q51scmmUatHqW+LvpU6jCEvL0+TJ0/W2rVrlZCQIEmKjY3VF198oYULF6pr164aPHiwZs6cqT179qhJkyYqLCzUG2+8oUcffdSxnVGjRjn+HhMTo6efflr33ntviTOpFXHTTTc5PV+0aJFCQkL0448/qk2bNgoJCZEkNWjQQOHh4Y5+EyZM0MyZM9X//47Fpk2b6scff9TChQs1dOhQLV++XIWFhXrllVcUEBCg1q1ba9++fbrvvvvKralz585KSUmRJDVv3lzr16/X7Nmz1aNHD4WEhMhut6tmzZpO9VQFzuhWM18fX81NKvqfi03OZzmKn89JmsMYIwDW0L+/tGuX9Nln0vLlRX/u3EnIxXmryLUulWn79u06efKkevToodq1azser776qmOYwJ/+9CfFx8c7zur++9//1sGDB3XzzTc7trN27VpdffXVaty4serUqaM77rhDv/32m06ePHnetf3888+67bbbFBsbq6CgIMXExEiS9uzZU+Y6J06c0I4dO3TXXXc5vZ+nn37a8X4yMjJ02WWXKSAgwLFeccgvz9n9EhISlJGRUcF3duE4o+sG/eP7a+XAlaXOozsnaQ7z6AKwFl9f6YwxgcCFcNe1LsXjaFetWqXGjRs7LbPb7Y6/Dx48WMuXL1dKSoqWL1+upKQkNWjQQFLR7Ae9e/fWfffdp0mTJql+/fr64osvdNdddyk/P1+1atUq8bo+Pj4yZ81HferUKafnN9xwg6Kjo/XSSy+pUaNGKiwsVJs2bZSfn1/u+3nppZfUsWNHp2W+FhpSRNB1k/7x/dW3RV+uFgUAoALcda1Lq1atZLfbtWfPHnXt2rXMfoMGDdKjjz6qTZs2aeXKlXrhhRccyzZt2qTCwkLNnDlTPj5Fv1R/6623zvm6ISEh+uGHH5za0tPT5efnJ0n67bfftG3bNr300ktK/L+LO7/44gun/v7+/pKkgoL/TV0aFhamRo0a6ZdfftHgwYNLfe34+HgtW7ZMf/zxh+Os7saNG89Zb7Gz+23cuFHx8fEurVuZCLpu5Ovje95TiDGlCgDgYuSua13q1KmjsWPHavTo0SosLFSXLl2Uk5Oj9evXKygoSEOHDpVUNO62U6dOuuuuu1RQUKA+ffo4thEXF6dTp05p3rx5uuGGG7R+/XqnIFyaq666StOnT9err76qhIQEvfbaa/rhhx/Url07SVK9evXUoEEDvfjii4qIiNCePXscY2OLhYaGqmbNmlq9erUiIyMVEBCg4OBgPfnkk3rggQcUHByspKQk5eXl6ZtvvtGRI0f04IMPatCgQRo/fryGDRumcePGadeuXZoxY4ZLn9f69es1bdo09evXT2vWrNGKFSu0atWqinzklYIxul7IHVOqAADgCdx5rctTTz2lxx57TFOmTFF8fLySkpK0atUqx/RdxQYPHqxvv/1WN954o2rWrOlob9u2rWbNmqWpU6eqTZs2ev311zVlypRzvmavXr302GOP6eGHH9af//xnHTt2TEOGDHEs9/Hx0RtvvKFNmzapTZs2Gj16tKZPn+60jRo1aujZZ5/VwoUL1ahRI/Xt21eSdPfdd+vll1/W4sWLdemll6pr165asmSJ4/3Url1b77//vr7//nu1a9dO48eP19SpU136rMaMGaNvvvlG7dq109NPP61Zs2apV69eLq1bmWzm7IEfF7nc3FwFBwcrJydHQUFB7i6nhOIpVc7+X2zxwb1y4ErG+AIAPNoff/yhnTt3qmnTpk4XOlVEakZqiWtdooKiuNbFA8TExGjUqFFOM0ycj3N9T1zNawxd8CLumlIFAABPw7UucAVB14tw+2AAAP7nQq51wcWBoOtFuH0wAADwdLt27XJ3CQ5cjOZFuH0wAACA6wi6XqR4SpWzrzItZpNNUUFR3D4YAOAVuB4e51IZ3w+Crhfh9sEAACsovtnBhdz2FtZX/P0o/r6cD8boehluHwwA8Ha+vr6qW7euDh48KEmqVauWbLbSf1uJi48xRidPntTBgwdVt27dC7olMfPonsXT59Etxp3RAADezBij7OxsHT161N2lwEPVrVtX4eHhpf4niHl0LY4pVQAA3sxmsykiIkKhoaE6deqUu8uBh/Hz87ugM7nFCLoAAMBtfH19KyXQAKXhYjQAAABYEkEXAAAAlkTQBQAAgCURdAEAAGBJBF0AAABYEkEXAAAAlkTQBQAAgCURdAEAAGBJBF0AAABYEkEXAAAAlkTQBQAAgCURdAEAAGBJBF0AAABYEkEXAAAAlkTQBQAAgCURdAEAAGBJBF0AAABYEkEXAAAAlkTQBQAAgCURdAEAAGBJBF0AAABYEkEXAAAAlkTQBQAAgCURdAEAAGBJBF0AAABYEkEXAAAAlkTQBQAAgCURdAEAAGBJBF0AAABYEkEXAAAAlkTQBQAAgCVZKujGxMTIZrM5PZ555hl3lwUAAAA3qOHuAirbxIkTNWzYMMfzOnXquLEaAAAAuIvlgm6dOnUUHh7u7jIAAADgZpYauiBJzzzzjBo0aKB27dpp+vTpOn369Dn75+XlKTc31+kBAAAA72epM7oPPPCALr/8ctWvX19ffvmlxo0bp6ysLM2aNavMdaZMmaInn3yyGqsEAABAdbAZY4y7iziXlJQUTZ069Zx9MjIy1LJlyxLtixYt0j333KPjx4/LbreXum5eXp7y8vIcz3NzcxUVFaWcnBwFBQVdWPEAAACodLm5uQoODi43r3l80D106JB+++23c/aJjY2Vv79/ifYtW7aoTZs22rp1q1q0aOHS67n6wQEAAMA9XM1rHj90ISQkRCEhIee1bnp6unx8fBQaGlrJVQEAAMDTeXzQddWGDRv01VdfqXv37qpTp442bNig0aNH6/bbb1e9evXcXR4AAACqmWWCrt1u1xtvvKEnnnhCeXl5atq0qUaPHq0HH3zQ3aUBAADADSwTdC+//HJt3LjR3WUAAADAQ1huHl0AAABAIugCAADAoiwzdAEAYDEFBVJampSVJUVESImJkq+vu6sC4EUIugAAz5OaKo0cKe3b97+2yEhp7lypf3/31QXAqzB0AQDgWVJTpQEDnEOuJGVmFrWnprqnLgBeh6ALAPAcBQVFZ3JLu2lncduoUUX9AKAcBF0AgOdISyt5JvdMxkh79xb1A4ByEHQBAJ4jK6ty+wG4qBF0AQCeIyKicvsBuKgRdAEAniMxsWh2BZut9OU2mxQVVdQPAMpB0AUAeA5f36IpxKSSYbf4+Zw5zKcLwCUEXQCAZ+nfX1q5Umrc2Lk9MrKonXl0AbiIG0YAADxP//5S377cGQ3ABSHoAgA8k6+v1K2bu6sA4MUYugAAAABLIugCAADAkgi6AAAAsCSCLgAAACyJoAsAAABLIugCAADAkgi6AAAAsCSCLgAAACyJoAsAAABL4s5oqBYFhQVK25OmrGNZiqgTocQmifL14VaeAACg6hB0UeVSM1I1cvVI7cvd52iLDIrU3KS56h/f342VAQAAK2PoAqpUakaqBrw1wCnkSlJmbqYGvDVAqRmpbqoMAABYHUEXVaagsEAjV4+UkSmxrLht1OpRKigsqO7SAADARYCgiyqTtietxJncMxkZ7c3dq7Q9adVYFQAAuFgQdFFlso5lVWo/AACAiiDoospE1Imo1H4AAAAVQdBFlUlskqjIoEjZZCt1uU02RQVFKbFJYjVXBgAALgYEXVQZXx9fzU2aK0klwm7x8zlJc5hPFwAAVAmCLqpU//j+WjlwpRoHNXZqjwyK1MqBK5lHFwAAVBmbMabk3E8XsdzcXAUHBysnJ0dBQUHuLscyuDMaAACoLK7mNe6Mhmrh6+OrbjHd3F0GAAC4iDB0AQAAAJZE0AUAAIAlEXQBAABgSQRdAAAAWBJBFwAAAJZE0AUAAIAlEXQBAABgSQRdAAAAWBJBFwAAAJZE0AUAAIAlEXQBAABgSQRdAAAAWBJBFwAAAJZE0AUAAIAlEXQBAABgSQRdAAAAWBJBFwAAAJZE0AUAAIAlEXQBAABgSQRdAAAAWBJBFwAAAJZE0AUAAIAlEXQBAABgSQRdAAAAWJLXBN1JkyapU6dOqlWrlurWrVtqnz179uj6669XrVq1FBoaqoceekinT5+u3kIBAADgEWq4uwBX5efn6+abb1ZCQoJeeeWVEssLCgp0/fXXKzw8XF9++aWysrI0ZMgQ+fn5afLkyW6oGAAAAO5kM8YYdxdREUuWLNGoUaN09OhRp/aPPvpIvXv31v79+xUWFiZJeuGFF/TII4/o0KFD8vf3d2n7ubm5Cg4OVk5OjoKCgiq7fAAAAFwgV/Oa1wxdKM+GDRt06aWXOkKuJPXq1Uu5ubnasmVLmevl5eUpNzfX6QEAAADvZ5mgm52d7RRyJTmeZ2dnl7nelClTFBwc7HhERUVVaZ0AAACoHm4NuikpKbLZbOd8bN26tUprGDdunHJychyPvXv3VunrAQAAoHq49WK0MWPGKDk5+Zx9YmNjXdpWeHi4/vvf/zq1HThwwLGsLHa7XXa73aXXAAAAgPdwa9ANCQlRSEhIpWwrISFBkyZN0sGDBxUaGipJWrNmjYKCgtSqVatKeQ0AAAB4D6+ZXmzPnj06fPiw9uzZo4KCAqWnp0uS4uLiVLt2bfXs2VOtWrXSHXfcoWnTpik7O1uPPvqoRowYwRlbD1FQWKC0PWnKOpaliDoRSmySKF8fX3eXBQAALMprphdLTk7W0qVLS7R/9tln6tatmyRp9+7duu+++/T5558rMDBQQ4cO1TPPPKMaNVzP80wvVjVSM1I1cvVI7cvd52iLDIrU3KS56h/f342VAQAAb+NqXvOaoFtdCLqVLzUjVQPeGiAj56+aTTZJ0sqBKwm7AADAZRfdPLrwTAWFBRq5emSJkCvJ0TZq9SgVFBZUd2kAAMDiCLqoUml70pyGK5zNyGhv7l6l7UmrxqoAAMDFwGsuRoPnceXisqxjWS5ty9V+AOA2BQVSWpqUlSVFREiJiZIvF9QCnoygi/Pi6sVlEXUiXNqeq/0AwC1SU6WRI6V9Z/yGKjJSmjtX6s81BoCnYugCKqz44rKzhyRk5mZqwFsDlJqR6mhLbJKoyKBIx4VnZ7PJpqigKCU2SazSmgHgvKWmSgMGOIdcScrMLGpPTS19PQBuR9BFhVT04jJfH1/NTZorSSXCbvHzOUlzmE8XgGcqKCg6k1vaBEXFbaNGFfUD4HEIuqiQ87m4rH98f60cuFKNgxo79Y0MimRqMQCeLS2t5JncMxkj7d1b1A+Ax2GMLirkfC8u6x/fX31b9OXOaAC8S5aLF8q62g9AtSLookIu5OIyXx9fdYvpVskVAUAVinDxQllX+wGoVgxdQIWUd3GZJEXWieTiMgDWkJhYNLuCrYx/82w2KSqqqB8Aj0PQRYWc6+KyYr+f/l3vbnu3OssCgKrh61s0hZhUMuwWP58zh/l0AQ9F0EWFFV9cVr9m/VKXH/79cIlpxgDAa/XvL61cKTV2vqBWkZFF7cyjC3gsmzGlzZly8crNzVVwcLBycnIUFBTk7nI8VkFhgWLmxGjfsdKvRrbJpsigSO0cuZMLzgBYA3dGAzyGq3mNi9FwXtL2pJUZciXnaca4AA2AJfj6St26ubsKABXA0AWcl/OdZgwAAKC6EHRxXi5kmjEAAIDqwNAFnJfiacYyczNLvR1w8RhdphkDgFIw3heoFpzRxXk51zRjxc/nJM3hQjQAOFtqqhQTI3XvLg0aVPRnTExRO4BKRdDFeSueZqxxkPOUO5FBkVo5cKX6xzPlDgA4SU2VBgyQ9p11MW9mZlE7YReoVEwvdhamF6u4gsICpe1JU9axLEXUiVBik0TO5ALA2QoKis7cnh1yi9lsRXPz7tzJMAagHEwvhmrj6+PLFGIAUJ60tLJDriQZI+3dW9SPacyASsHQBQAAqkOWi9MtutoPQLkIugAAVIcIF6dbdLUfgHIRdAEAqA6JiUVjcG220pfbbFJUVFE/AJWCoAsAQHXw9ZXmFk3LWCLsFj+fM4cL0YBKRNAFAKC69O8vrVwpNXaellGRkUXt/ZmWEahMzLoAAEB16t9f6tuXO6MB1YCgCwBAdfP1ZQoxoBowdAEAAACWRNAFAACAJTF0AQAAeK6CAsYz47wRdAEAgGdKTZVGjnS+dXJkZNE0bcxQARcwdAEAAHie1FRpwADnkCtJmZlF7amp7qkLXoWgCwAAPEtBQdGZXGNKLituGzWqqB9wDgRdAADgWdLSSp7JPZMx0t69Rf2AcyDoAgAAz5KVVbn9cNEi6AIAAM8SEVG5/XDRIugCAADPkphYNLuCzVb6cptNiooq6gecA0EXAAB4Fl/foinEpJJht/j5nDnMp4tyEXQBAIDn6d9fWrlSatzYuT0ysqideXThAm4YAQAAPFP//lLfvtwZDeeNoAsAADyXr6/UrZu7q4CXYugCAAAALImgCwAAAEsi6AIAAMCSCLoAAACwJIIuAAAALKlCQTcrK0uvvfaaPvzwQ+Xn5zstO3HihCZOnFipxQEAAADny2aMMa50/Prrr9WzZ08VFhbq1KlTaty4sd555x21bt1aknTgwAE1atRIBQUFVVpwVcvNzVVwcLBycnIUFBTk7nIAAABwFlfzmstndP/xj3/oxhtv1JEjR3TgwAH16NFDXbt21ebNmyulYAAAAKAyuXzDiE2bNun555+Xj4+P6tSpo/nz56tJkya6+uqr9fHHH6tJkyZVWScAAABQIRW6M9off/zh9DwlJUU1atRQz549tWjRokotDAAAALgQLgfdNm3a6Msvv9Rll13m1D527FgVFhbqtttuq/TiAAAAgPPl8hjdIUOGaP369aUue/jhh/Xkk08yfAEAAAAew+VZFy4WzLoAAADg2Sp91gUAAADAmxB0AQAAYEkEXQAAAFgSQRcAAACW5DVBd9KkSerUqZNq1aqlunXrltrHZrOVeLzxxhvVWygAAAA8gkvz6D777LP629/+poCAAD377LPn7Fu7dm21bt1aHTt2rJQCi+Xn5+vmm29WQkKCXnnllTL7LV68WElJSY7nZYViAAAAWJtLQXf27NkaPHiwAgICNHv27HP2zcvL08GDBzV69GhNnz69UoqUpCeffFKStGTJknP2q1u3rsLDwyvtdQEAAOCdXAq6O3fuLPXvZVmzZo0GDRpUqUHXVSNGjNDdd9+t2NhY3Xvvvbrzzjtls9nK7J+Xl6e8vDzH89zc3OooEwAAAFXM5VsAV0SXLl306KOPVsWmz2nixIm66qqrVKtWLX3yyScaPny4jh8/rgceeKDMdaZMmeI4WwwAAADrOK87o3366aeaPXu2MjIyJEnx8fEaNWqUrrnmmgptJyUlRVOnTj1nn4yMDLVs2dLxfMmSJRo1apSOHj1a7vYff/xxLV68WHv37i2zT2lndKOiorgzGgAAgIdy9c5oFT6jO3/+fI0cOVIDBgzQyJEjJUkbN27Uddddp9mzZ2vEiBEub2vMmDFKTk4+Z5/Y2NiKlujQsWNHPfXUU8rLy5Pdbi+1j91uL3MZAAAAvFeFg+7kyZM1e/Zs3X///Y62Bx54QJ07d9bkyZMrFHRDQkIUEhJS0RJclp6ernr16hFkAQAALkIVDrpHjx51mr6rWM+ePfXII49USlGl2bNnjw4fPqw9e/aooKBA6enpkqS4uDjVrl1b77//vg4cOKC//OUvCggI0Jo1azR58mSNHTu2ymoCAACA56pw0O3Tp4/+9a9/6aGHHnJqf/fdd9W7d+9KK+xsjz/+uJYuXep43q5dO0nSZ599pm7dusnPz0/PP/+8Ro8eLWOM4uLiNGvWLA0bNqzKagIAAIDnculitDNvEpGbm6sZM2aoc+fOSkhIkFQ0Rnf9+vUaM2aMW2ZbqEyuDm4GAACAe7ia11wKuk2bNnXpRW02m3755RfXq/RABF0AAADPVqmzLrhykwgAAADAk/ic74q//vqrfv3118qsBQAAAKg0FQq6R48e1YgRI9SwYUOFhYUpLCxMDRs21P333+/SDRwAAACA6uLyrAuHDx9WQkKCMjMzNXjwYMXHx0uSfvzxRy1ZskSffvqpvvzyS9WrV6/KigUAAABc5XLQnThxovz9/bVjxw6FhYWVWNazZ09NnDhRs2fPrvQiAQAAgIpyeejCO++8oxkzZpQIuZIUHh6uadOm6V//+lelFgcAAACcL5eDblZWllq3bl3m8jZt2ig7O7tSigIAAAAulMtBt2HDhtq1a1eZy3fu3Kn69etXRk0AAADABXM56Pbq1Uvjx49Xfn5+iWV5eXl67LHHlJSUVKnFAQAAAOfLpTujSdK+ffvUoUMH2e12jRgxQi1btpQxRhkZGZo/f77y8vL0zTffKCoqqqprrlLcGQ0AAMCzVeqd0SQpMjJSGzZs0PDhwzVu3DgV52ObzaYePXroueee8/qQCwAAAOtwOehKUtOmTfXRRx/pyJEj+vnnnyVJcXFxjM0FAACAx6lQ0C1Wr149XXHFFZVdCwAAAFBpKnQLYAAAAMBbEHQBAABgSQRdAAAAWBJBFwAAAJZE0AUAAIAlEXQBAABgSQRdAAAAWBJBFwAAAJZE0AUAAIAlEXQBAABgSQRdAAAAWBJBFwAAAJZE0AUAAIAlEXQBAABgSQRdAAAAWBJBFwAAAJZE0AUAAIAlEXQBAABgSQRdAAAAWBJBFwAAAJZE0AUAAIAlEXQBAABgSQRdAAAAWBJBFwAAAJZE0AUAAIAlEXQBAABgSQRdAAAAWBJBFwAAAJZE0AUAAIAlEXQBAABgSQRdAAAAWBJBFwAAAJZE0AUAAIAlEXQBAABgSQRdAAAAWBJBFwAAAJZE0AUAAIAlEXQBAABgSQRdAAAAWBJBFwAAAJZE0AUAAIAlEXQBAABgSQRdAAAAWBJBFwAAAJZE0AUAAIAleUXQ3bVrl+666y41bdpUNWvWVLNmzTRhwgTl5+c79fvuu++UmJiogIAARUVFadq0aW6qGAAAAO5Ww90FuGLr1q0qLCzUwoULFRcXpx9++EHDhg3TiRMnNGPGDElSbm6uevbsqWuuuUYvvPCCvv/+e/31r39V3bp19be//c3N7wAAAADVzWaMMe4u4nxMnz5dCxYs0C+//CJJWrBggcaPH6/s7Gz5+/tLklJSUvTOO+9o69atLm83NzdXwcHBysnJUVBQUJXUDgAAgPPnal7ziqELpcnJyVH9+vUdzzds2KArr7zSEXIlqVevXtq2bZuOHDlS5nby8vKUm5vr9AAAAID388qgu337ds2bN0/33HOPoy07O1thYWFO/YqfZ2dnl7mtKVOmKDg42PGIioqqmqIBAABQrdwadFNSUmSz2c75OHvYQWZmppKSknTzzTdr2LBhF1zDuHHjlJOT43js3bv3grcJAAAA93PrxWhjxoxRcnLyOfvExsY6/r5//351795dnTp10osvvujULzw8XAcOHHBqK34eHh5e5vbtdrvsdnsFKwcAAICnc2vQDQkJUUhIiEt9MzMz1b17d7Vv316LFy+Wj4/zyeiEhASNHz9ep06dkp+fnyRpzZo1atGiherVq1fptQMAAMCzecUY3czMTHXr1k1NmjTRjBkzdOjQIWVnZzuNvR00aJD8/f111113acuWLXrzzTc1d+5cPfjgg26sHAAAAO7iFfPorlmzRtu3b9f27dsVGRnptKx4drTg4GB98sknGjFihNq3b6+GDRvq8ccfZw5dAACAi5TXzqNbVZhHFwAAwLNZfh5dAAAA4FwIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugAAALAkgi4AAAAsySuC7q5du3TXXXepadOmqlmzppo1a6YJEyYoPz/fqY/NZivx2LhxoxsrBwAAgLvUcHcBrti6dasKCwu1cOFCxcXF6YcfftCwYcN04sQJzZgxw6nv2rVr1bp1a8fzBg0aVHe5AAAA8ABeEXSTkpKUlJTkeB4bG6tt27ZpwYIFJYJugwYNFB4eXt0lAgAAwMN4xdCF0uTk5Kh+/fol2vv06aPQ0FB16dJF7733XrnbycvLU25urtMDAAAA3s8rg+727ds1b9483XPPPY622rVra+bMmVqxYoVWrVqlLl26qF+/fuWG3SlTpig4ONjxiIqKquryAQAAUA1sxhjjrhdPSUnR1KlTz9knIyNDLVu2dDzPzMxU165d1a1bN7388svnXHfIkCHauXOn0tLSyuyTl5envLw8x/Pc3FxFRUUpJydHQUFBLr4TAAAAVJfc3FwFBweXm9fcOkZ3zJgxSk5OPmef2NhYx9/379+v7t27q1OnTnrxxRfL3X7Hjh21Zs2ac/ax2+2y2+0u1QsAAADv4dagGxISopCQEJf6ZmZmqnv37mrfvr0WL14sH5/yR12kp6crIiLiQssEAACAF/KKWRcyMzPVrVs3RUdHa8aMGTp06JBjWfEMC0uXLpW/v7/atWsnSUpNTdWiRYvKHd4AAAAAa/KKoLtmzRpt375d27dvV2RkpNOyM4cYP/XUU9q9e7dq1Kihli1b6s0339SAAQOqu1wAAAB4ALdejOaJXB3cDAAAAPdwNa955fRiAAAAQHkIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugAAALAkgi4AAAAsiaALAAAASyLoAgAAwJIIugAAALCkGu4uABevgsICpe1JU9axLEXUiVBik0T5+vi6uywAAGARBF24RWpGqkauHql9ufscbZFBkZqbNFf94/u7sTIAAGAVDF1AtUvNSNWAtwY4hVxJyszN1IC3Big1I9VNlQEAACsh6KJaFRQWaOTqkTIyJZYVt41aPUoFhQXVXRoAALAYgi6qVdqetBJncs9kZLQ3d6/S9qRVY1UAAMCKCLqoVlnHsiq1HwAAQFkIuqhWEXUiKrUfAABAWQi6qFaJTRIVGRQpm2ylLrfJpqigKCU2SazmygAAgNUQdFGtfH18NTdpriSVCLvFz+ckzWE+XQAAcMEIuqh2/eP7a+XAlWoc1NipPTIoUisHrmQeXQAAUClsxpiS8zxdxHJzcxUcHKycnBwFBQW5uxxL485oAADgfLia17gzGtzG18dX3WK6ubsMAABgUQxdAAAAgCURdAEAAGBJBF0AAABYEkEXAAAAlkTQBQAAgCURdAEAAGBJBF0AAABYEkEXAAAAlkTQBQAAgCURdAEAAGBJBF0AAABYEkEXAAAAlkTQBQAAgCURdAEAAGBJBF0AAABYEkEXAAAAlkTQBQAAgCURdAEAAGBJBF0AAABYEkEXAAAAlkTQBQAAgCURdAEAAGBJBF0AAABYEkEXAAAAlkTQBQAAgCURdAEAAGBJBF0AAABYEkEXAAAAlkTQBQAAgCURdAEAAGBJBF0AAABYEkEXAAAAllTD3QUAAADASxUUSGlpUlaWFBEhJSZKvr7ursrBa87o9unTR02aNFFAQIAiIiJ0xx13aP/+/U59vvvuOyUmJiogIEBRUVGaNm2am6oFAACwuNRUKSZG6t5dGjSo6M+YmKJ2D+E1Qbd79+566623tG3bNr399tvasWOHBgwY4Fiem5urnj17Kjo6Wps2bdL06dP1xBNP6MUXX3Rj1QAAABaUmioNGCDt2+fcnplZ1O4hYddmjDHuLuJ8vPfee+rXr5/y8vLk5+enBQsWaPz48crOzpa/v78kKSUlRe+88462bt3q8nZzc3MVHBysnJwcBQUFVVX5AAAA3qmgoOjM7dkht5jNJkVGSjt3VtkwBlfzmtec0T3T4cOH9frrr6tTp07y8/OTJG3YsEFXXnmlI+RKUq9evbRt2zYdOXKkzG3l5eUpNzfX6QEAAIAypKWVHXIlyRhp796ifm7mVUH3kUceUWBgoBo0aKA9e/bo3XffdSzLzs5WWFiYU//i59nZ2WVuc8qUKQoODnY8oqKiqqZ4AAAAK8jKqtx+VcitQTclJUU2m+2cjzOHHTz00EPavHmzPvnkE/n6+mrIkCG60JEX48aNU05OjuOxd+/eC31bAAAA1hURUbn9qpBbpxcbM2aMkpOTz9knNjbW8feGDRuqYcOGat68ueLj4xUVFaWNGzcqISFB4eHhOnDggNO6xc/Dw8PL3L7dbpfdbj//NwEAAHAxSUwsGoObmVk0TOFsxWN0ExOrv7azuDXohoSEKCQk5LzWLSwslFQ0xlaSEhISNH78eJ06dcoxbnfNmjVq0aKF6tWrVzkFAwAAXOx8faW5c4tmV7DZnMOuzVb055w5HjGfrleM0f3qq6/03HPPKT09Xbt379a6det02223qVmzZkpISJAkDRo0SP7+/rrrrru0ZcsWvfnmm5o7d64efPBBN1cPAABgMf37SytXSo0bO7dHRha19+/vnrrO4hXTi33//fcaOXKkvv32W504cUIRERFKSkrSo48+qsZnfMDfffedRowYoa+//loNGzbU3//+dz3yyCMVei2mFwMAAHCRm+6M5mpe84qgW50IugAAAJ7N0vPoAgAAAOUh6AIAAMCSCLoAAACwJIIuAAAALImgCwAAAEsi6AIAAMCSCLoAAACwJIIuAAAALImgCwAAAEsi6AIAAMCSCLoAAACwJIIuAAAALImgCwAAAEuq4e4CPI0xRpKUm5vr5koAAABQmuKcVpzbykLQPcuxY8ckSVFRUW6uBAAAAOdy7NgxBQcHl7ncZsqLwheZwsJC7d+/X3Xq1JHNZjvv7eTm5ioqKkp79+5VUFBQJVaI6sR+tAb2o/djH1oD+9EaPGE/GmN07NgxNWrUSD4+ZY/E5YzuWXx8fBQZGVlp2wsKCuJgtgD2ozWwH70f+9Aa2I/W4O79eK4zucW4GA0AAACWRNAFAACAJRF0q4jdbteECRNkt9vdXQouAPvRGtiP3o99aA3sR2vwpv3IxWgAAACwJM7oAgAAwJIIugAAALAkgi4AAAAsiaALAAAASyLoVoKYmBjZbDanxzPPPONYvmvXrhLLbTabNm7c6LSdFStWqGXLlgoICNCll16qDz/8sLrfykWrvH0oSd99950SExMVEBCgqKgoTZs2rcR22IeeIS8vT3/6059ks9mUnp7uaOdY9C5l7UeJ49Eb9OnTR02aNFFAQIAiIiJ0xx13aP/+/Y7lHI+er7x9KHnBsWhwwaKjo83EiRNNVlaW43H8+HHH8p07dxpJZu3atU598vPzHX3Wr19vfH19zbRp08yPP/5oHn30UePn52e+//57d7yli055+zAnJ8eEhYWZwYMHmx9++MH885//NDVr1jQLFy509GEfeo4HHnjAXHvttUaS2bx5s6OdY9G7lLUfOR69w6xZs8yGDRvMrl27zPr1601CQoJJSEhwLOd49Hzl7UNvOBYJupUgOjrazJ49u8zlxQfzmf9Qn23gwIHm+uuvd2rr2LGjueeeeyqpSpxLeftw/vz5pl69eiYvL8/R9sgjj5gWLVo4nrMPPcOHH35oWrZsabZs2VJm0OVY9Hzn2o8cj97p3XffNTabzRFkOR69z9n70BuORYYuVJJnnnlGDRo0ULt27TR9+nSdPn26RJ8+ffooNDRUXbp00Xvvvee0bMOGDbrmmmuc2nr16qUNGzZUad34n3Ptww0bNujKK6+Uv7+/o61Xr17atm2bjhw54ujDPnSvAwcOaNiwYVq2bJlq1apVZj+ORc9W3n7kePQ+hw8f1uuvv65OnTrJz8/PaRnHo3cobR96w7FI0K0EDzzwgN544w199tlnuueeezR58mQ9/PDDjuW1a9fWzJkztWLFCq1atUpdunRRv379nA7o7OxshYWFOW03LCxM2dnZ1fY+Lmbl7cOy9k/xsnP1YR9WD2OMkpOTde+996pDhw6l9uFY9Hyu7EeOR+/xyCOPKDAwUA0aNNCePXv07rvvOpZxPHqHc+1DbzgWCbplSElJKXWQ/JmPrVu3SpIefPBBdevWTZdddpnuvfdezZw5U/PmzVNeXp4kqWHDhnrwwQfVsWNH/fnPf9Yzzzyj22+/XdOnT3fnW7S8ytyHcB9X9+O8efN07NgxjRs3rsxtcSy6T2XuR7hPRf5dlaSHHnpImzdv1ieffCJfX18NGTJE5v9uyMrx6B6VuQ+9QQ13F+CpxowZo+Tk5HP2iY2NLbW9Y8eOOn36tHbt2qUWLVqU2WfNmjWO5+Hh4Tpw4IBTnwMHDig8PLxihcOhMvdhWftHkmMfsQ+rhqv7cd26ddqwYUOJe6936NBBgwcP1tKlS0tdl2OxelTmfuR4dJ+K/rvasGFDNWzYUM2bN1d8fLyioqK0ceNGJSQklLoux2PVq8x96A3HIkG3DCEhIQoJCTmvddPT0+Xj46PQ0NBz9omIiHA8T0hI0KeffqpRo0Y52tasWVPmPwYoX2Xuw4SEBI0fP16nTp1yjE1as2aNWrRooXr16jn6sA8rn6v78dlnn9XTTz/teL5//3716tVLb775pjp27FjmehyL1aMy9yPHo/tcyL+rhYWFknTO35RxPFa9ytyHXnEsVsslbxb25ZdfmtmzZ5v09HSzY8cO89prr5mQkBAzZMgQR58lS5aY5cuXm4yMDJORkWEmTZpkfHx8zKJFixx91q9fb2rUqGFmzJhhMjIyzIQJE5hCpZq4sg+PHj1qwsLCzB133GF++OEH88Ybb5hatWqVmEKFfeg5Sruim2PR+5S2HzkePd/GjRvNvHnzzObNm82uXbvMp59+ajp16mSaNWtm/vjjD2MMx6Onc2UfesOxSNC9QJs2bTIdO3Y0wcHBJiAgwMTHx5vJkyc7vgTGFB3M8fHxplatWiYoKMhcccUVZsWKFSW29dZbb5nmzZsbf39/07p1a7Nq1arqfCsXLVf2oTHGfPvtt6ZLly7Gbrebxo0bm2eeeabEttiHnqOsoMux6F3KmoKK49Gzfffdd6Z79+6mfv36xm63m5iYGHPvvfeaffv2OfpwPHo2V/ahMZ5/LNqM8aIRxQAAAICLmHUBAAAAlkTQBQAAgCURdAEAAGBJBF0AAABYEkEXAAAAlkTQBQAAgCURdAEAAGBJBF0AAABYEkEXACwiJiZGc+bMueA+AGAVBF0A8HB79+7VX//6VzVq1Ej+/v6Kjo7WyJEj9dtvv1V4W19//bX+9re/lbl8y5YtuummmxQTEyObzUYoBuDVCLoA4MF++eUXdejQQT///LP++c9/avv27XrhhRf06aefKiEhQYcPH67Q9kJCQlSrVq0yl588eVKxsbF65plnFB4efqHlA4BbEXQBwIONGDFC/v7++uSTT9S1a1c1adJE1157rdauXavMzEyNHz/eqf+xY8d02223KTAwUI0bN9bzzz/vtLy8oQt//vOfNX36dN16662y2+1V8ZYAoNoQdAHAQx0+fFgff/yxhg8frpo1azotCw8P1+DBg/Xmm2/KGONonz59utq2bavNmzcrJSVFI0eO1Jo1a6q7dADwCDXcXQAAoHQ///yzjDGKj48vdXl8fLyOHDmiQ4cOKTQ0VJLUuXNnpaSkSJKaN2+u9evXa/bs2erRo0e11Q0AnoIzugDg4c48Y1uehISEEs8zMjIquyQA8AoEXQDwUHFxcbLZbGUG1YyMDNWrV08hISHVXBkAeAeCLgB4qAYNGqhHjx6aP3++fv/9d6dl2dnZev3113XLLbfIZrM52jdu3OjUb+PGjWUOfQAAqyPoAoAHe+6555SXl6devXrpP//5j/bu3avVq1erR48eaty4sSZNmuTUf/369Zo2bZp++uknPf/881qxYoVGjhzp8uvl5+crPT1d6enpys/PV2ZmptLT07V9+/bKfmsAUOUIugDgwS655BJ98803io2N1cCBA9WsWTP97W9/U/fu3bVhwwbVr1/fqf+YMWP0zTffqF27dnr66ac1a9Ys9erVy+XX279/v9q1a6d27dopKytLM2bMULt27XT33XdX9lsDgCrHrAsA4OGio6O1ZMmScvvt2rWr3D55eXmqXbt2mctjYmIqdPEbAHgygi4AXAROnjyp9evX68CBA2rdurW7ywGAasHQBQC4CLz44ou69dZbNWrUqBJTkAGAVdkMv6MCAACABXFGFwAAAJZE0AUAAIAlEXQBAABgSQRdAAAAWBJBFwAAAJZE0AUAAIAlEXQBAABgSQRdAAAAWNL/B5ojq8ZymbB8AAAAAElFTkSuQmCC\n"},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":606,"status":"ok","timestamp":1749205407563,"user":{"displayName":"Liam Mertens","userId":"17654526473594897979"},"user_tz":-120},"id":"oM5zEvS0yLdx","outputId":"8760c30d-58e0-4274-c551-9063ed7b0af5"},"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: content/drive/MyDrive/mo_awr/Results/ (stored 0%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/ (stored 0%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_1.png (deflated 7%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_2.png (deflated 7%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_3.png (deflated 7%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_4.png (deflated 7%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_5.png (deflated 7%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_6.png (deflated 5%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_7.png (deflated 6%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_8.png (deflated 6%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_9.png (deflated 7%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_10.png (deflated 7%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_11.png (deflated 7%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_12.png (deflated 7%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_13.png (deflated 6%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_14.png (deflated 7%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_15.png (deflated 7%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_16.png (deflated 7%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_17.png (deflated 7%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_18.png (deflated 8%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_19.png (deflated 7%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_20.png (deflated 6%)\n","  adding: content/drive/MyDrive/mo_awr/Results/plots/training_plot_21.png (deflated 7%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/ (stored 0%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_1.csv (deflated 22%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_2.csv (deflated 20%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_3.csv (deflated 37%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_4.csv (deflated 50%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_5.csv (deflated 46%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_6.csv (deflated 49%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_7.csv (deflated 48%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_8.csv (deflated 48%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_9.csv (deflated 49%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_10.csv (deflated 22%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_11.csv (deflated 49%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_12.csv (deflated 49%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_13.csv (deflated 49%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_14.csv (deflated 48%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_15.csv (deflated 47%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_16.csv (deflated 48%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_17.csv (deflated 49%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_18.csv (deflated 49%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_19.csv (deflated 48%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_20.csv (deflated 36%)\n","  adding: content/drive/MyDrive/mo_awr/Results/data/points_21.csv (deflated 47%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/ (stored 0%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_1.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_1.pt (deflated 18%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_1.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_2.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_2.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_2.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_3.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_3.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_3.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_4.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_4.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_4.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_5.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_5.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_5.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_6.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_6.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_6.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_7.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_7.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_7.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_8.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_8.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_8.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_9.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_9.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_9.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_10.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_10.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_10.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_11.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_11.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_11.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_12.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_12.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_12.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_13.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_13.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_13.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_14.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_14.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_14.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_15.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_15.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_15.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_16.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_16.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_16.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_17.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_17.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_17.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_18.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_18.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_18.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_19.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_19.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_19.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_20.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_20.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_20.pt (deflated 16%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/value_21.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/policy_21.pt (deflated 17%)\n","  adding: content/drive/MyDrive/mo_awr/Results/weights/popf_21.pt (deflated 16%)\n"]}],"source":["!zip -r /content/drive/MyDrive/mo_awr/results.zip /content/drive/MyDrive/mo_awr/Results/"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":513,"status":"ok","timestamp":1749369818655,"user":{"displayName":"Liam Mertens","userId":"17654526473594897979"},"user_tz":-120},"id":"VqQrwUz-rcS5"},"outputs":[],"source":["!rm -rf /content/drive/MyDrive/mo_awr/Results/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":190771,"status":"ok","timestamp":1749140969479,"user":{"displayName":"Liam Mertens","userId":"17654526473594897979"},"user_tz":-120},"id":"LoLHOFW8VPpr","outputId":"065e10da-df20-4f47-b99b-5d1ffd457b54"},"outputs":[{"data":{"text/plain":["[[array([11.311, 14.566]), 25.0],\n"," [array([10.738, 15.798]), 25.0],\n"," [array([ 6.747, 17.644]), 25.0],\n"," [array([12.968, 13.264]), 25.0],\n"," [array([ 6.275, 17.485]), 25.0]]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["agent.evaluate_pf(64)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":240,"status":"ok","timestamp":1749325846365,"user":{"displayName":"Liam Mertens","userId":"17654526473594897979"},"user_tz":-120},"id":"5STgWNBbuZrj","outputId":"4c17ca47-62d0-423a-e66f-ee5fb836f23f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(-4.925318,\n","  13348,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.411, -5.366], dtype=float32), observed_return=array([ 0.7, -6. ], dtype=float32), horizon=6, next_observation=array([0, 1], dtype=int32), prob=0.3229936, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.403, -4.288], dtype=float32), observed_return=array([ 0.7, -5. ], dtype=float32), horizon=5, next_observation=array([0, 1], dtype=int32), prob=0.11773526, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.414, -3.258], dtype=float32), observed_return=array([ 0.7, -4. ], dtype=float32), horizon=4, next_observation=array([0, 2], dtype=int32), prob=0.2767935, terminal=False),\n","   Transition(observation=array([0, 2], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.499, -2.601], dtype=float32), observed_return=array([ 0.7, -3. ], dtype=float32), horizon=3, next_observation=array([0, 1], dtype=int32), prob=0.10160083, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.559, -1.71 ], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([1, 1], dtype=int32), prob=0.61926395, terminal=False),\n","   Transition(observation=array([1, 1], dtype=int32), action=2, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.65504575, terminal=True)]),\n"," (-4.7207856,\n","  13333,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.541, -7.184], dtype=float32), observed_return=array([ 8.2, -7. ], dtype=float32), horizon=7, next_observation=array([0, 1], dtype=int32), prob=0.3229936, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.602, -5.988], dtype=float32), observed_return=array([ 8.2, -6. ], dtype=float32), horizon=6, next_observation=array([0, 2], dtype=int32), prob=0.30434084, terminal=False),\n","   Transition(observation=array([0, 2], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.516, -5.053], dtype=float32), observed_return=array([ 8.2, -5. ], dtype=float32), horizon=5, next_observation=array([0, 1], dtype=int32), prob=0.10065414, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.555, -3.893], dtype=float32), observed_return=array([ 8.2, -4. ], dtype=float32), horizon=4, next_observation=array([1, 1], dtype=int32), prob=0.6108154, terminal=False),\n","   Transition(observation=array([1, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.565, -2.843], dtype=float32), observed_return=array([ 8.2, -3. ], dtype=float32), horizon=3, next_observation=array([1, 2], dtype=int32), prob=0.15639803, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.403, -1.86 ], dtype=float32), observed_return=array([ 8.2, -2. ], dtype=float32), horizon=2, next_observation=array([2, 2], dtype=int32), prob=0.70868677, terminal=False),\n","   Transition(observation=array([2, 2], dtype=int32), action=2, reward=array([ 8.2, -1. ], dtype=float32), return_=array([ 8.2, -1. ], dtype=float32), observed_return=array([ 8.2, -1. ], dtype=float32), horizon=1, next_observation=array([2, 1], dtype=int32), prob=0.7509924, terminal=True)]),\n"," (-1.3526534,\n","  13338,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.313, -4.192], dtype=float32), observed_return=array([ 0.7, -5. ], dtype=float32), horizon=5, next_observation=array([0, 1], dtype=int32), prob=0.3229936, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.313, -3.152], dtype=float32), observed_return=array([ 0.7, -4. ], dtype=float32), horizon=4, next_observation=array([1, 1], dtype=int32), prob=0.4499897, terminal=False),\n","   Transition(observation=array([1, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.408, -2.254], dtype=float32), observed_return=array([ 0.7, -3. ], dtype=float32), horizon=3, next_observation=array([1, 2], dtype=int32), prob=0.42692012, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.559, -1.71 ], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([1, 1], dtype=int32), prob=0.102106065, terminal=False),\n","   Transition(observation=array([1, 1], dtype=int32), action=2, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.22927052, terminal=True)]),\n"," (-0.31010354,\n","  13361,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([11.611, -5.648], dtype=float32), observed_return=array([11.5, -5. ], dtype=float32), horizon=5, next_observation=array([0, 1], dtype=int32), prob=0.3229936, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([11.695, -4.406], dtype=float32), observed_return=array([11.5, -4. ], dtype=float32), horizon=4, next_observation=array([1, 1], dtype=int32), prob=0.4499897, terminal=False),\n","   Transition(observation=array([1, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([11.762, -3.25 ], dtype=float32), observed_return=array([11.5, -3. ], dtype=float32), horizon=3, next_observation=array([1, 2], dtype=int32), prob=0.42692012, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([11.632, -2.073], dtype=float32), observed_return=array([11.5, -2. ], dtype=float32), horizon=2, next_observation=array([2, 2], dtype=int32), prob=0.65696275, terminal=False),\n","   Transition(observation=array([2, 2], dtype=int32), action=1, reward=array([11.5, -1. ], dtype=float32), return_=array([11.5, -1. ], dtype=float32), observed_return=array([11.5, -1. ], dtype=float32), horizon=1, next_observation=array([3, 2], dtype=int32), prob=0.5976012, terminal=True)]),\n"," (-4.481424,\n","  13326,\n","  [Transition(observation=array([0, 0], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 12.368, -10.771], dtype=float32), observed_return=array([ 11.5, -10. ], dtype=float32), horizon=10, next_observation=array([0, 0], dtype=int32), prob=0.19316225, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([12.594, -9.49 ], dtype=float32), observed_return=array([11.5, -9. ], dtype=float32), horizon=9, next_observation=array([0, 1], dtype=int32), prob=0.40917054, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([12.78 , -8.195], dtype=float32), observed_return=array([11.5, -8. ], dtype=float32), horizon=8, next_observation=array([0, 2], dtype=int32), prob=0.27074522, terminal=False),\n","   Transition(observation=array([0, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([12.81 , -7.099], dtype=float32), observed_return=array([11.5, -7. ], dtype=float32), horizon=7, next_observation=array([1, 2], dtype=int32), prob=0.70004, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([12.754, -5.875], dtype=float32), observed_return=array([11.5, -6. ], dtype=float32), horizon=6, next_observation=array([2, 2], dtype=int32), prob=0.7138957, terminal=False),\n","   Transition(observation=array([2, 2], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([12.67 , -4.768], dtype=float32), observed_return=array([11.5, -5. ], dtype=float32), horizon=5, next_observation=array([2, 3], dtype=int32), prob=0.040412307, terminal=False),\n","   Transition(observation=array([2, 3], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([12.451, -3.693], dtype=float32), observed_return=array([11.5, -4. ], dtype=float32), horizon=4, next_observation=array([2, 4], dtype=int32), prob=0.028233975, terminal=False),\n","   Transition(observation=array([2, 4], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([12.119, -2.761], dtype=float32), observed_return=array([11.5, -3. ], dtype=float32), horizon=3, next_observation=array([3, 4], dtype=int32), prob=0.73136914, terminal=False),\n","   Transition(observation=array([3, 4], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([11.752, -1.914], dtype=float32), observed_return=array([11.5, -2. ], dtype=float32), horizon=2, next_observation=array([3, 3], dtype=int32), prob=0.6997546, terminal=False),\n","   Transition(observation=array([3, 3], dtype=int32), action=2, reward=array([11.5, -1. ], dtype=float32), return_=array([11.5, -1. ], dtype=float32), observed_return=array([11.5, -1. ], dtype=float32), horizon=1, next_observation=array([3, 2], dtype=int32), prob=0.8383296, terminal=True)]),\n"," (-0.29124174,\n","  8716,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.621, -1.871], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.24775279, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.354604, terminal=True)]),\n"," (-0.23921388,\n","  3852,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.673, -1.875], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.11817428, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.18836503, terminal=True)]),\n"," (-0.22188838,\n","  173,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.7, -3. ], dtype=float32), observed_return=array([ 0.7, -3. ], dtype=float32), horizon=3, next_observation=array([0, 0], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.7, -2. ], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.25, terminal=True)]),\n"," (-0.18856646,\n","  12652,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.611, -1.98 ], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.23064935, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.3351557, terminal=True)]),\n"," (-0.3047639,\n","  13341,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.231, -3.165], dtype=float32), observed_return=array([ 8.2, -3. ], dtype=float32), horizon=3, next_observation=array([0, 1], dtype=int32), prob=0.3229936, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.224, -2.033], dtype=float32), observed_return=array([ 8.2, -2. ], dtype=float32), horizon=2, next_observation=array([1, 1], dtype=int32), prob=0.4499897, terminal=False),\n","   Transition(observation=array([1, 1], dtype=int32), action=1, reward=array([ 8.2, -1. ], dtype=float32), return_=array([ 8.2, -1. ], dtype=float32), observed_return=array([ 8.2, -1. ], dtype=float32), horizon=1, next_observation=array([2, 1], dtype=int32), prob=0.2893257, terminal=True)]),\n"," (-0.2583457,\n","  10692,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.584, -1.945], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.6266148, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.169862, terminal=True)]),\n"," (-0.2583457,\n","  10666,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.584, -1.945], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.6266148, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.169862, terminal=True)]),\n"," (-0.27709973,\n","  11071,\n","  [Transition(observation=array([0, 0], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.601, -2.082], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.20648557, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.12349494, terminal=True)]),\n"," (-0.16860794,\n","  7599,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.439, -3.058], dtype=float32), observed_return=array([ 8.2, -3. ], dtype=float32), horizon=3, next_observation=array([0, 1], dtype=int32), prob=0.54725975, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.342, -1.983], dtype=float32), observed_return=array([ 8.2, -2. ], dtype=float32), horizon=2, next_observation=array([1, 1], dtype=int32), prob=0.39778638, terminal=False),\n","   Transition(observation=array([1, 1], dtype=int32), action=1, reward=array([ 8.2, -1. ], dtype=float32), return_=array([ 8.2, -1. ], dtype=float32), observed_return=array([ 8.2, -1. ], dtype=float32), horizon=1, next_observation=array([2, 1], dtype=int32), prob=0.16739942, terminal=True)]),\n"," (-0.2147234,\n","  7961,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.668, -2.978], dtype=float32), observed_return=array([ 0.7, -3. ], dtype=float32), horizon=3, next_observation=array([0, 0], dtype=int32), prob=0.27261278, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.668, -1.924], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.5395651, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.24916674, terminal=True)]),\n"," (-0.09004974,\n","  7229,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.685, -1.95 ], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.16328396, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.12409041, terminal=True)]),\n"," (-2e-05,\n","  1996,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.27281243, terminal=True)]),\n"," (-0.17240387,\n","  2402,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.665, -1.914], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.21083908, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.26749858, terminal=True)]),\n"," (-2e-05,\n","  2409,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.30281493, terminal=True)]),\n"," (-0.21010537,\n","  9708,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.62 , -1.929], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.4398203, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.07422526, terminal=True)]),\n"," (-0.29106152,\n","  2007,\n","  [Transition(observation=array([0, 0], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.621, -1.872], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.2752562, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.27544844, terminal=True)]),\n"," (-0.18856646,\n","  12664,\n","  [Transition(observation=array([0, 0], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.611, -1.98 ], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.20126012, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.31809887, terminal=True)]),\n"," (-0.15251419,\n","  7958,\n","  [Transition(observation=array([0, 0], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.668, -1.924], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.19628878, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.286703, terminal=True)]),\n"," (-2e-05,\n","  152,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.25, terminal=True)]),\n"," (-0.18867032,\n","  9030,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.668, -2.077], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.24460435, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.3896556, terminal=True)]),\n"," (-0.24527657,\n","  526,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.643, -2.815], dtype=float32), observed_return=array([ 0.7, -3. ], dtype=float32), horizon=3, next_observation=array([0, 0], dtype=int32), prob=0.2755604, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.661, -1.942], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.27709052, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.27099004, terminal=True)]),\n"," (-0.06995864,\n","  8284,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.67 , -1.986], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.8227696, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.13256793, terminal=True)]),\n"," (-2e-05,\n","  527,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.2853422, terminal=True)]),\n"," (-2e-05,\n","  166,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.25, terminal=True)]),\n"," (-2e-05,\n","  535,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.2853422, terminal=True)]),\n"," (-2e-05,\n","  837,\n","  [Transition(observation=array([0, 0], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.705, -1.99 ], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.21814103, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.25768363, terminal=True)]),\n"," (-0.021287022,\n","  9,\n","  [Transition(observation=array([0, 0], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.7, -2. ], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.25, terminal=True)]),\n"," (-2e-05,\n","  857,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.705, -1.99 ], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.25618914, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.25881156, terminal=True)]),\n"," (-2e-05,\n","  13354,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.26320803, terminal=True)]),\n"," (-2e-05,\n","  4622,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.17301144, terminal=True)]),\n"," (-0.021287022,\n","  176,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.7, -2. ], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.25, terminal=True)]),\n"," (-2e-05,\n","  925,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.26586905, terminal=True)]),\n"," (-2e-05,\n","  6192,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.17875856, terminal=True)]),\n"," (-2e-05,\n","  2410,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.30281493, terminal=True)]),\n"," (-0.021287022,\n","  196,\n","  [Transition(observation=array([0, 0], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.7, -2. ], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.25, terminal=True)]),\n"," (-2e-05,\n","  2717,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.36214343, terminal=True)]),\n"," (-0.28791267,\n","  3045,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.638, -1.863], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.1191768, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.13111737, terminal=True)]),\n"," (-2e-05,\n","  140,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.25, terminal=True)]),\n"," (-2e-05,\n","  16,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.25, terminal=True)]),\n"," (-0.1834727,\n","  13356,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.629, -2.042], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.2206361, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.24369465, terminal=True)]),\n"," (-2e-05,\n","  6911,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.19557023, terminal=True)]),\n"," (-0.1049711,\n","  1269,\n","  [Transition(observation=array([0, 0], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.764, -2.944], dtype=float32), observed_return=array([ 0.7, -3. ], dtype=float32), horizon=3, next_observation=array([0, 0], dtype=int32), prob=0.2517691, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.712, -1.99 ], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.256487, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.27995503, terminal=True)]),\n"," (-2e-05,\n","  7607,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.1498423, terminal=True)]),\n"," (-2e-05,\n","  517,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.736, -2.895], dtype=float32), observed_return=array([ 0.7, -3. ], dtype=float32), horizon=3, next_observation=array([0, 1], dtype=int32), prob=0.2419372, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.711, -1.987], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([1, 1], dtype=int32), prob=0.28024173, terminal=False),\n","   Transition(observation=array([1, 1], dtype=int32), action=2, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.29946765, terminal=True)]),\n"," (-2e-05,\n","  6881,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.19557023, terminal=True)]),\n"," (-2e-05,\n","  518,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.2853422, terminal=True)]),\n"," (-2e-05,\n","  10,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.25, terminal=True)]),\n"," (-2e-05,\n","  3521,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.23167515, terminal=True)]),\n"," (-2e-05,\n","  1613,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.2869926, terminal=True)]),\n"," (-2e-05,\n","  1612,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.2869926, terminal=True)]),\n"," (-2e-05,\n","  4296,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.1560213, terminal=True)]),\n"," (-2e-05,\n","  1614,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.2869926, terminal=True)]),\n"," (-2e-05,\n","  3883,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.18680672, terminal=True)]),\n"," (-2e-05,\n","  1646,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.2869926, terminal=True)]),\n"," (-2e-05,\n","  5350,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.23950858, terminal=True)]),\n"," (-2e-05,\n","  1950,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.27281243, terminal=True)]),\n"," (-2e-05,\n","  12285,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.3012532, terminal=True)]),\n"," (-2e-05,\n","  1972,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.27281243, terminal=True)]),\n"," (-2e-05,\n","  6177,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.17875856, terminal=True)]),\n"," (-2e-05,\n","  5355,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.23950858, terminal=True)]),\n"," (-0.0,\n","  13353,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([11.746, -5.571], dtype=float32), observed_return=array([11.5, -5. ], dtype=float32), horizon=5, next_observation=array([0, 1], dtype=int32), prob=0.3229936, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([11.845, -4.321], dtype=float32), observed_return=array([11.5, -4. ], dtype=float32), horizon=4, next_observation=array([0, 2], dtype=int32), prob=0.30434084, terminal=False),\n","   Transition(observation=array([0, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([11.762, -3.25 ], dtype=float32), observed_return=array([11.5, -3. ], dtype=float32), horizon=3, next_observation=array([1, 2], dtype=int32), prob=0.62988704, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([11.632, -2.073], dtype=float32), observed_return=array([11.5, -2. ], dtype=float32), horizon=2, next_observation=array([2, 2], dtype=int32), prob=0.6849539, terminal=False),\n","   Transition(observation=array([2, 2], dtype=int32), action=1, reward=array([11.5, -1. ], dtype=float32), return_=array([11.5, -1. ], dtype=float32), observed_return=array([11.5, -1. ], dtype=float32), horizon=1, next_observation=array([3, 2], dtype=int32), prob=0.5976012, terminal=True)]),\n"," (-2e-05,\n","  7982,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.2840364, terminal=True)]),\n"," (-0.0,\n","  174,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.25, terminal=True)]),\n"," (-0.0,\n","  12658,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.684, -4.828], dtype=float32), observed_return=array([ 8.2, -5. ], dtype=float32), horizon=5, next_observation=array([0, 1], dtype=int32), prob=0.26583695, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.732, -3.776], dtype=float32), observed_return=array([ 8.2, -4. ], dtype=float32), horizon=4, next_observation=array([0, 2], dtype=int32), prob=0.20823076, terminal=False),\n","   Transition(observation=array([0, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.601, -2.871], dtype=float32), observed_return=array([ 8.2, -3. ], dtype=float32), horizon=3, next_observation=array([1, 2], dtype=int32), prob=0.6503946, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.427, -1.894], dtype=float32), observed_return=array([ 8.2, -2. ], dtype=float32), horizon=2, next_observation=array([2, 2], dtype=int32), prob=0.27749473, terminal=False),\n","   Transition(observation=array([2, 2], dtype=int32), action=2, reward=array([ 8.2, -1. ], dtype=float32), return_=array([ 8.2, -1. ], dtype=float32), observed_return=array([ 8.2, -1. ], dtype=float32), horizon=1, next_observation=array([2, 1], dtype=int32), prob=0.8578929, terminal=True)]),\n"," (-2e-05,\n","  6554,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.2187751, terminal=True)]),\n"," (-2e-05,\n","  12270,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.3012532, terminal=True)]),\n"," (-2e-05,\n","  924,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.26586905, terminal=True)]),\n"," (-0.0,\n","  2400,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.597, -29.371], dtype=float32), observed_return=array([  8.2, -59. ], dtype=float32), horizon=59, next_observation=array([0, 0], dtype=int32), prob=0.21083908, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.748, -29.418], dtype=float32), observed_return=array([  8.2, -58. ], dtype=float32), horizon=58, next_observation=array([0, 1], dtype=int32), prob=0.3108953, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.754, -29.383], dtype=float32), observed_return=array([  8.2, -57. ], dtype=float32), horizon=57, next_observation=array([0, 2], dtype=int32), prob=0.26205325, terminal=False),\n","   Transition(observation=array([0, 2], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.7  , -29.312], dtype=float32), observed_return=array([  8.2, -56. ], dtype=float32), horizon=56, next_observation=array([0, 2], dtype=int32), prob=0.1790585, terminal=False),\n","   Transition(observation=array([0, 2], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.643, -29.235], dtype=float32), observed_return=array([  8.2, -55. ], dtype=float32), horizon=55, next_observation=array([0, 3], dtype=int32), prob=0.23544835, terminal=False),\n","   Transition(observation=array([0, 3], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.549, -29.127], dtype=float32), observed_return=array([  8.2, -54. ], dtype=float32), horizon=54, next_observation=array([0, 4], dtype=int32), prob=0.2105413, terminal=False),\n","   Transition(observation=array([0, 4], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.445, -29.007], dtype=float32), observed_return=array([  8.2, -53. ], dtype=float32), horizon=53, next_observation=array([1, 4], dtype=int32), prob=0.541339, terminal=False),\n","   Transition(observation=array([1, 4], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.38 , -28.926], dtype=float32), observed_return=array([  8.2, -52. ], dtype=float32), horizon=52, next_observation=array([2, 4], dtype=int32), prob=0.5498629, terminal=False),\n","   Transition(observation=array([2, 4], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.379, -28.903], dtype=float32), observed_return=array([  8.2, -51. ], dtype=float32), horizon=51, next_observation=array([3, 4], dtype=int32), prob=0.46890426, terminal=False),\n","   Transition(observation=array([3, 4], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.469, -28.963], dtype=float32), observed_return=array([  8.2, -50. ], dtype=float32), horizon=50, next_observation=array([3, 5], dtype=int32), prob=0.2292141, terminal=False),\n","   Transition(observation=array([3, 5], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.521, -28.995], dtype=float32), observed_return=array([  8.2, -49. ], dtype=float32), horizon=49, next_observation=array([3, 6], dtype=int32), prob=0.20355846, terminal=False),\n","   Transition(observation=array([3, 6], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.566, -29.022], dtype=float32), observed_return=array([  8.2, -48. ], dtype=float32), horizon=48, next_observation=array([3, 7], dtype=int32), prob=0.17505658, terminal=False),\n","   Transition(observation=array([3, 7], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.615, -29.047], dtype=float32), observed_return=array([  8.2, -47. ], dtype=float32), horizon=47, next_observation=array([3, 8], dtype=int32), prob=0.14703321, terminal=False),\n","   Transition(observation=array([3, 8], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.65 , -29.053], dtype=float32), observed_return=array([  8.2, -46. ], dtype=float32), horizon=46, next_observation=array([4, 8], dtype=int32), prob=0.65303886, terminal=False),\n","   Transition(observation=array([4, 8], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.732, -29.108], dtype=float32), observed_return=array([  8.2, -45. ], dtype=float32), horizon=45, next_observation=array([3, 8], dtype=int32), prob=0.08788877, terminal=False),\n","   Transition(observation=array([3, 8], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.753, -29.095], dtype=float32), observed_return=array([  8.2, -44. ], dtype=float32), horizon=44, next_observation=array([3, 7], dtype=int32), prob=0.17135666, terminal=False),\n","   Transition(observation=array([3, 7], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.775, -29.078], dtype=float32), observed_return=array([  8.2, -43. ], dtype=float32), horizon=43, next_observation=array([4, 7], dtype=int32), prob=0.47418454, terminal=False),\n","   Transition(observation=array([4, 7], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.819, -29.084], dtype=float32), observed_return=array([  8.2, -42. ], dtype=float32), horizon=42, next_observation=array([4, 6], dtype=int32), prob=0.22130811, terminal=False),\n","   Transition(observation=array([4, 6], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.849, -29.071], dtype=float32), observed_return=array([  8.2, -41. ], dtype=float32), horizon=41, next_observation=array([4, 7], dtype=int32), prob=0.24724849, terminal=False),\n","   Transition(observation=array([4, 7], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.871, -29.045], dtype=float32), observed_return=array([  8.2, -40. ], dtype=float32), horizon=40, next_observation=array([5, 7], dtype=int32), prob=0.3289401, terminal=False),\n","   Transition(observation=array([5, 7], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.898, -29.017], dtype=float32), observed_return=array([  8.2, -39. ], dtype=float32), horizon=39, next_observation=array([5, 6], dtype=int32), prob=0.22382604, terminal=False),\n","   Transition(observation=array([5, 6], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 17.011, -29.038], dtype=float32), observed_return=array([  8.2, -38. ], dtype=float32), horizon=38, next_observation=array([5, 7], dtype=int32), prob=0.2601458, terminal=False),\n","   Transition(observation=array([5, 7], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 17.022, -28.978], dtype=float32), observed_return=array([  8.2, -37. ], dtype=float32), horizon=37, next_observation=array([4, 7], dtype=int32), prob=0.23673432, terminal=False),\n","   Transition(observation=array([4, 7], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 17.007, -28.884], dtype=float32), observed_return=array([  8.2, -36. ], dtype=float32), horizon=36, next_observation=array([3, 7], dtype=int32), prob=0.23542526, terminal=False),\n","   Transition(observation=array([3, 7], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.961, -28.746], dtype=float32), observed_return=array([  8.2, -35. ], dtype=float32), horizon=35, next_observation=array([2, 7], dtype=int32), prob=0.22775976, terminal=False),\n","   Transition(observation=array([2, 7], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.874, -28.556], dtype=float32), observed_return=array([  8.2, -34. ], dtype=float32), horizon=34, next_observation=array([3, 7], dtype=int32), prob=0.23536903, terminal=False),\n","   Transition(observation=array([3, 7], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.795, -28.364], dtype=float32), observed_return=array([  8.2, -33. ], dtype=float32), horizon=33, next_observation=array([3, 6], dtype=int32), prob=0.2674431, terminal=False),\n","   Transition(observation=array([3, 6], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.709, -28.154], dtype=float32), observed_return=array([  8.2, -32. ], dtype=float32), horizon=32, next_observation=array([2, 6], dtype=int32), prob=0.24300428, terminal=False),\n","   Transition(observation=array([2, 6], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.594, -27.899], dtype=float32), observed_return=array([  8.2, -31. ], dtype=float32), horizon=31, next_observation=array([2, 7], dtype=int32), prob=0.26097435, terminal=False),\n","   Transition(observation=array([2, 7], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.452, -27.601], dtype=float32), observed_return=array([  8.2, -30. ], dtype=float32), horizon=30, next_observation=array([3, 7], dtype=int32), prob=0.21032746, terminal=False),\n","   Transition(observation=array([3, 7], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.302, -27.278], dtype=float32), observed_return=array([  8.2, -29. ], dtype=float32), horizon=29, next_observation=array([3, 8], dtype=int32), prob=0.24748355, terminal=False),\n","   Transition(observation=array([3, 8], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.122, -26.905], dtype=float32), observed_return=array([  8.2, -28. ], dtype=float32), horizon=28, next_observation=array([3, 7], dtype=int32), prob=0.2970574, terminal=False),\n","   Transition(observation=array([3, 7], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 15.928, -26.497], dtype=float32), observed_return=array([  8.2, -27. ], dtype=float32), horizon=27, next_observation=array([3, 6], dtype=int32), prob=0.29614562, terminal=False),\n","   Transition(observation=array([3, 6], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 15.717, -26.051], dtype=float32), observed_return=array([  8.2, -26. ], dtype=float32), horizon=26, next_observation=array([4, 6], dtype=int32), prob=0.21717423, terminal=False),\n","   Transition(observation=array([4, 6], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 15.482, -25.557], dtype=float32), observed_return=array([  8.2, -25. ], dtype=float32), horizon=25, next_observation=array([3, 6], dtype=int32), prob=0.24956262, terminal=False),\n","   Transition(observation=array([3, 6], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 15.217, -25.001], dtype=float32), observed_return=array([  8.2, -24. ], dtype=float32), horizon=24, next_observation=array([2, 6], dtype=int32), prob=0.23840049, terminal=False),\n","   Transition(observation=array([2, 6], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 14.918, -24.378], dtype=float32), observed_return=array([  8.2, -23. ], dtype=float32), horizon=23, next_observation=array([2, 5], dtype=int32), prob=0.32887602, terminal=False),\n","   Transition(observation=array([2, 5], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 14.6  , -23.704], dtype=float32), observed_return=array([  8.2, -22. ], dtype=float32), horizon=22, next_observation=array([3, 5], dtype=int32), prob=0.19105127, terminal=False),\n","   Transition(observation=array([3, 5], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 14.243, -22.953], dtype=float32), observed_return=array([  8.2, -21. ], dtype=float32), horizon=21, next_observation=array([2, 5], dtype=int32), prob=0.24265693, terminal=False),\n","   Transition(observation=array([2, 5], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 13.85 , -22.122], dtype=float32), observed_return=array([  8.2, -20. ], dtype=float32), horizon=20, next_observation=array([1, 5], dtype=int32), prob=0.2347382, terminal=False),\n","   Transition(observation=array([1, 5], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 13.426, -21.214], dtype=float32), observed_return=array([  8.2, -19. ], dtype=float32), horizon=19, next_observation=array([1, 4], dtype=int32), prob=0.35393485, terminal=False),\n","   Transition(observation=array([1, 4], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 13.008, -20.273], dtype=float32), observed_return=array([  8.2, -18. ], dtype=float32), horizon=18, next_observation=array([0, 4], dtype=int32), prob=0.2520556, terminal=False),\n","   Transition(observation=array([0, 4], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 12.628, -19.339], dtype=float32), observed_return=array([  8.2, -17. ], dtype=float32), horizon=17, next_observation=array([0, 3], dtype=int32), prob=0.37625727, terminal=False),\n","   Transition(observation=array([0, 3], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 12.32 , -18.462], dtype=float32), observed_return=array([  8.2, -16. ], dtype=float32), horizon=16, next_observation=array([0, 4], dtype=int32), prob=0.21882209, terminal=False),\n","   Transition(observation=array([0, 4], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 11.917, -17.414], dtype=float32), observed_return=array([  8.2, -15. ], dtype=float32), horizon=15, next_observation=array([0, 5], dtype=int32), prob=0.20347136, terminal=False),\n","   Transition(observation=array([0, 5], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 11.407, -16.179], dtype=float32), observed_return=array([  8.2, -14. ], dtype=float32), horizon=14, next_observation=array([0, 4], dtype=int32), prob=0.40112156, terminal=False),\n","   Transition(observation=array([0, 4], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 10.971, -15.008], dtype=float32), observed_return=array([  8.2, -13. ], dtype=float32), horizon=13, next_observation=array([1, 4], dtype=int32), prob=0.14034554, terminal=False),\n","   Transition(observation=array([1, 4], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 10.421, -13.636], dtype=float32), observed_return=array([  8.2, -12. ], dtype=float32), horizon=12, next_observation=array([1, 3], dtype=int32), prob=0.371501, terminal=False),\n","   Transition(observation=array([1, 3], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([  9.97, -12.36], dtype=float32), observed_return=array([  8.2, -11. ], dtype=float32), horizon=11, next_observation=array([1, 2], dtype=int32), prob=0.3451633, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([  9.707, -11.246], dtype=float32), observed_return=array([  8.2, -10. ], dtype=float32), horizon=10, next_observation=array([2, 2], dtype=int32), prob=0.20150807, terminal=False),\n","   Transition(observation=array([2, 2], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 9.285, -9.856], dtype=float32), observed_return=array([ 8.2, -9. ], dtype=float32), horizon=9, next_observation=array([1, 2], dtype=int32), prob=0.21814708, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 9.053, -8.71 ], dtype=float32), observed_return=array([ 8.2, -8. ], dtype=float32), horizon=8, next_observation=array([0, 2], dtype=int32), prob=0.23542084, terminal=False),\n","   Transition(observation=array([0, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.886, -7.676], dtype=float32), observed_return=array([ 8.2, -7. ], dtype=float32), horizon=7, next_observation=array([1, 2], dtype=int32), prob=0.16787556, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.719, -6.592], dtype=float32), observed_return=array([ 8.2, -6. ], dtype=float32), horizon=6, next_observation=array([1, 3], dtype=int32), prob=0.25487876, terminal=False),\n","   Transition(observation=array([1, 3], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.435, -5.425], dtype=float32), observed_return=array([ 8.2, -5. ], dtype=float32), horizon=5, next_observation=array([1, 2], dtype=int32), prob=0.34979013, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.355, -4.375], dtype=float32), observed_return=array([ 8.2, -4. ], dtype=float32), horizon=4, next_observation=array([1, 3], dtype=int32), prob=0.25539356, terminal=False),\n","   Transition(observation=array([1, 3], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.172, -3.227], dtype=float32), observed_return=array([ 8.2, -3. ], dtype=float32), horizon=3, next_observation=array([1, 2], dtype=int32), prob=0.35046512, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.206, -2.17 ], dtype=float32), observed_return=array([ 8.2, -2. ], dtype=float32), horizon=2, next_observation=array([2, 2], dtype=int32), prob=0.19997965, terminal=False),\n","   Transition(observation=array([2, 2], dtype=int32), action=2, reward=array([ 8.2, -1. ], dtype=float32), return_=array([ 8.2, -1. ], dtype=float32), observed_return=array([ 8.2, -1. ], dtype=float32), horizon=1, next_observation=array([2, 1], dtype=int32), prob=0.2986878, terminal=True)]),\n"," (-2e-05,\n","  11959,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.16920325, terminal=True)]),\n"," (-2e-05,\n","  2403,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.30281493, terminal=True)]),\n"," (-0.0,\n","  932,\n","  [Transition(observation=array([0, 0], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.705, -1.99 ], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.25618914, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.25881156, terminal=True)]),\n"," (-2e-05,\n","  11414,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.14009415, terminal=True)]),\n"," (-2e-05,\n","  9028,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.31348366, terminal=True)]),\n"," (-2e-05,\n","  2411,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.30281493, terminal=True)]),\n"," (-2e-05,\n","  2716,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.36214343, terminal=True)]),\n"," (-0.0,\n","  11044,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([11.875, -6.846], dtype=float32), observed_return=array([11.5, -7. ], dtype=float32), horizon=7, next_observation=array([0, 1], dtype=int32), prob=0.53128636, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([12.079, -5.626], dtype=float32), observed_return=array([11.5, -6. ], dtype=float32), horizon=6, next_observation=array([0, 2], dtype=int32), prob=0.36871368, terminal=False),\n","   Transition(observation=array([0, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([12.12 , -4.597], dtype=float32), observed_return=array([11.5, -5. ], dtype=float32), horizon=5, next_observation=array([1, 2], dtype=int32), prob=0.52723664, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([12.061, -3.506], dtype=float32), observed_return=array([11.5, -4. ], dtype=float32), horizon=4, next_observation=array([1, 3], dtype=int32), prob=0.35405946, terminal=False),\n","   Transition(observation=array([1, 3], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([11.886, -2.668], dtype=float32), observed_return=array([11.5, -3. ], dtype=float32), horizon=3, next_observation=array([2, 3], dtype=int32), prob=0.76959586, terminal=False),\n","   Transition(observation=array([2, 3], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([11.653, -1.899], dtype=float32), observed_return=array([11.5, -2. ], dtype=float32), horizon=2, next_observation=array([2, 2], dtype=int32), prob=0.28933805, terminal=False),\n","   Transition(observation=array([2, 2], dtype=int32), action=1, reward=array([11.5, -1. ], dtype=float32), return_=array([11.5, -1. ], dtype=float32), observed_return=array([11.5, -1. ], dtype=float32), horizon=1, next_observation=array([3, 2], dtype=int32), prob=0.43121392, terminal=True)]),\n"," (-2e-05,\n","  12662,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.30225348, terminal=True)]),\n"," (-2e-05,\n","  2718,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.36214343, terminal=True)]),\n"," (-2e-05,\n","  2719,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.36214343, terminal=True)]),\n"," (-2e-05,\n","  8714,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.31706542, terminal=True)]),\n"," (-2e-05,\n","  2724,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.36214343, terminal=True)]),\n"," (-2e-05,\n","  12331,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.3012532, terminal=True)]),\n"," (-2e-05,\n","  9018,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.31348366, terminal=True)]),\n"," (-0.0,\n","  13001,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 15.958, -10.016], dtype=float32), observed_return=array([16.1, -9. ], dtype=float32), horizon=9, next_observation=array([0, 1], dtype=int32), prob=0.34488586, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([16.173, -8.774], dtype=float32), observed_return=array([16.1, -8. ], dtype=float32), horizon=8, next_observation=array([1, 1], dtype=int32), prob=0.37725744, terminal=False),\n","   Transition(observation=array([1, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([16.409, -7.646], dtype=float32), observed_return=array([16.1, -7. ], dtype=float32), horizon=7, next_observation=array([1, 2], dtype=int32), prob=0.47783753, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([16.474, -6.372], dtype=float32), observed_return=array([16.1, -6. ], dtype=float32), horizon=6, next_observation=array([1, 3], dtype=int32), prob=0.28770876, terminal=False),\n","   Transition(observation=array([1, 3], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([16.458, -5.213], dtype=float32), observed_return=array([16.1, -5. ], dtype=float32), horizon=5, next_observation=array([1, 4], dtype=int32), prob=0.07667892, terminal=False),\n","   Transition(observation=array([1, 4], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([16.393, -4.091], dtype=float32), observed_return=array([16.1, -4. ], dtype=float32), horizon=4, next_observation=array([1, 5], dtype=int32), prob=0.029549249, terminal=False),\n","   Transition(observation=array([1, 5], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([16.299, -3.01 ], dtype=float32), observed_return=array([16.1, -3. ], dtype=float32), horizon=3, next_observation=array([2, 5], dtype=int32), prob=0.8782576, terminal=False),\n","   Transition(observation=array([2, 5], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([16.199, -2.013], dtype=float32), observed_return=array([16.1, -2. ], dtype=float32), horizon=2, next_observation=array([3, 5], dtype=int32), prob=0.6616109, terminal=False),\n","   Transition(observation=array([3, 5], dtype=int32), action=1, reward=array([16.1, -1. ], dtype=float32), return_=array([16.1, -1. ], dtype=float32), observed_return=array([16.1, -1. ], dtype=float32), horizon=1, next_observation=array([4, 5], dtype=int32), prob=0.5319326, terminal=True)]),\n"," (-2e-05,\n","  1266,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.2719194, terminal=True)]),\n"," (-2e-05,\n","  6138,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.17875856, terminal=True)]),\n"," (-2e-05,\n","  13016,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.2421809, terminal=True)]),\n"," (-0.0,\n","  514,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.736, -2.895], dtype=float32), observed_return=array([ 0.7, -3. ], dtype=float32), horizon=3, next_observation=array([0, 1], dtype=int32), prob=0.2419372, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.711, -1.987], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([1, 1], dtype=int32), prob=0.28024173, terminal=False),\n","   Transition(observation=array([1, 1], dtype=int32), action=2, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.29946765, terminal=True)]),\n"," (-2e-05,\n","  12992,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.2421809, terminal=True)]),\n"," (-0.074894376,\n","  1272,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.841, -3.007], dtype=float32), observed_return=array([ 0.7, -3. ], dtype=float32), horizon=3, next_observation=array([0, 1], dtype=int32), prob=0.2529095, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.712, -1.99 ], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([0, 0], dtype=int32), prob=0.20297684, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.2846458, terminal=True)]),\n"," (-2e-05,\n","  11926,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.16920325, terminal=True)]),\n"," (-0.0,\n","  5385,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.474, -2.981], dtype=float32), observed_return=array([ 8.2, -3. ], dtype=float32), horizon=3, next_observation=array([0, 1], dtype=int32), prob=0.31748274, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.339, -1.949], dtype=float32), observed_return=array([ 8.2, -2. ], dtype=float32), horizon=2, next_observation=array([1, 1], dtype=int32), prob=0.4024851, terminal=False),\n","   Transition(observation=array([1, 1], dtype=int32), action=1, reward=array([ 8.2, -1. ], dtype=float32), return_=array([ 8.2, -1. ], dtype=float32), observed_return=array([ 8.2, -1. ], dtype=float32), horizon=1, next_observation=array([2, 1], dtype=int32), prob=0.3772464, terminal=True)]),\n"," (-2e-05,\n","  1286,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.2719194, terminal=True)]),\n"," (-2e-05,\n","  12330,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.3012532, terminal=True)]),\n"," (-2e-05,\n","  9031,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.31348366, terminal=True)]),\n"," (-2e-05,\n","  12653,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.30225348, terminal=True)]),\n"," (-2e-05,\n","  7943,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.2840364, terminal=True)]),\n"," (-2e-05,\n","  7592,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.1498423, terminal=True)]),\n"," (-0.0,\n","  7969,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([11.661, -4.924], dtype=float32), observed_return=array([11.5, -5. ], dtype=float32), horizon=5, next_observation=array([0, 1], dtype=int32), prob=0.24706206, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([11.708, -3.853], dtype=float32), observed_return=array([11.5, -4. ], dtype=float32), horizon=4, next_observation=array([1, 1], dtype=int32), prob=0.41228336, terminal=False),\n","   Transition(observation=array([1, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([11.705, -2.834], dtype=float32), observed_return=array([11.5, -3. ], dtype=float32), horizon=3, next_observation=array([1, 2], dtype=int32), prob=0.04753063, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([11.613, -1.864], dtype=float32), observed_return=array([11.5, -2. ], dtype=float32), horizon=2, next_observation=array([2, 2], dtype=int32), prob=0.39615345, terminal=False),\n","   Transition(observation=array([2, 2], dtype=int32), action=1, reward=array([11.5, -1. ], dtype=float32), return_=array([11.5, -1. ], dtype=float32), observed_return=array([11.5, -1. ], dtype=float32), horizon=1, next_observation=array([3, 2], dtype=int32), prob=0.22931635, terminal=True)]),\n"," (-2e-05,\n","  10657,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.23169681, terminal=True)]),\n"," (-0.0,\n","  5819,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.543, -15.717], dtype=float32), observed_return=array([ 20.3, -16. ], dtype=float32), horizon=16, next_observation=array([0, 1], dtype=int32), prob=0.6614636, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.639, -14.773], dtype=float32), observed_return=array([ 20.3, -15. ], dtype=float32), horizon=15, next_observation=array([0, 2], dtype=int32), prob=0.43889385, terminal=False),\n","   Transition(observation=array([0, 2], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.717, -13.827], dtype=float32), observed_return=array([ 20.3, -14. ], dtype=float32), horizon=14, next_observation=array([0, 3], dtype=int32), prob=0.403616, terminal=False),\n","   Transition(observation=array([0, 3], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.781, -12.904], dtype=float32), observed_return=array([ 20.3, -13. ], dtype=float32), horizon=13, next_observation=array([1, 3], dtype=int32), prob=0.2317676, terminal=False),\n","   Transition(observation=array([1, 3], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 16.897, -12.111], dtype=float32), observed_return=array([ 20.3, -12. ], dtype=float32), horizon=12, next_observation=array([2, 3], dtype=int32), prob=0.20336601, terminal=False),\n","   Transition(observation=array([2, 3], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 17.051, -11.348], dtype=float32), observed_return=array([ 20.3, -11. ], dtype=float32), horizon=11, next_observation=array([2, 4], dtype=int32), prob=0.366338, terminal=False),\n","   Transition(observation=array([2, 4], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 17.207, -10.546], dtype=float32), observed_return=array([ 20.3, -10. ], dtype=float32), horizon=10, next_observation=array([2, 5], dtype=int32), prob=0.33645773, terminal=False),\n","   Transition(observation=array([2, 5], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([17.373, -9.71 ], dtype=float32), observed_return=array([20.3, -9. ], dtype=float32), horizon=9, next_observation=array([3, 5], dtype=int32), prob=0.44297472, terminal=False),\n","   Transition(observation=array([3, 5], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([17.595, -8.87 ], dtype=float32), observed_return=array([20.3, -8. ], dtype=float32), horizon=8, next_observation=array([3, 6], dtype=int32), prob=0.2522346, terminal=False),\n","   Transition(observation=array([3, 6], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([17.832, -7.967], dtype=float32), observed_return=array([20.3, -7. ], dtype=float32), horizon=7, next_observation=array([3, 7], dtype=int32), prob=0.15645674, terminal=False),\n","   Transition(observation=array([3, 7], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([18.087, -6.972], dtype=float32), observed_return=array([20.3, -6. ], dtype=float32), horizon=6, next_observation=array([3, 8], dtype=int32), prob=0.09700002, terminal=False),\n","   Transition(observation=array([3, 8], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([18.364, -5.863], dtype=float32), observed_return=array([20.3, -5. ], dtype=float32), horizon=5, next_observation=array([4, 8], dtype=int32), prob=0.7614154, terminal=False),\n","   Transition(observation=array([4, 8], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([18.732, -4.75 ], dtype=float32), observed_return=array([20.3, -4. ], dtype=float32), horizon=4, next_observation=array([4, 7], dtype=int32), prob=0.24301815, terminal=False),\n","   Transition(observation=array([4, 7], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([19.196, -3.616], dtype=float32), observed_return=array([20.3, -3. ], dtype=float32), horizon=3, next_observation=array([5, 7], dtype=int32), prob=0.5382061, terminal=False),\n","   Transition(observation=array([5, 7], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([19.72 , -2.377], dtype=float32), observed_return=array([20.3, -2. ], dtype=float32), horizon=2, next_observation=array([6, 7], dtype=int32), prob=0.32231817, terminal=False),\n","   Transition(observation=array([6, 7], dtype=int32), action=1, reward=array([20.3, -1. ], dtype=float32), return_=array([20.3, -1. ], dtype=float32), observed_return=array([20.3, -1. ], dtype=float32), horizon=1, next_observation=array([7, 7], dtype=int32), prob=0.20217203, terminal=True)]),\n"," (-0.0,\n","  11062,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([13.886, -7.211], dtype=float32), observed_return=array([14., -7.], dtype=float32), horizon=7, next_observation=array([0, 1], dtype=int32), prob=0.53128636, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([14.158, -5.958], dtype=float32), observed_return=array([14., -6.], dtype=float32), horizon=6, next_observation=array([1, 1], dtype=int32), prob=0.3632619, terminal=False),\n","   Transition(observation=array([1, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([14.295, -4.665], dtype=float32), observed_return=array([14., -5.], dtype=float32), horizon=5, next_observation=array([1, 2], dtype=int32), prob=0.54265684, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([14.309, -3.53 ], dtype=float32), observed_return=array([14., -4.], dtype=float32), horizon=4, next_observation=array([1, 3], dtype=int32), prob=0.2887868, terminal=False),\n","   Transition(observation=array([1, 3], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([14.233, -2.639], dtype=float32), observed_return=array([14., -3.], dtype=float32), horizon=3, next_observation=array([2, 3], dtype=int32), prob=0.76959586, terminal=False),\n","   Transition(observation=array([2, 3], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([14.128, -1.822], dtype=float32), observed_return=array([14., -2.], dtype=float32), horizon=2, next_observation=array([3, 3], dtype=int32), prob=0.56171644, terminal=False),\n","   Transition(observation=array([3, 3], dtype=int32), action=1, reward=array([14., -1.], dtype=float32), return_=array([14., -1.], dtype=float32), observed_return=array([14., -1.], dtype=float32), horizon=1, next_observation=array([4, 3], dtype=int32), prob=0.37995726, terminal=True)]),\n"," (-2e-05,\n","  5376,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.23950858, terminal=True)]),\n"," (-0.0,\n","  11080,\n","  [Transition(observation=array([0, 0], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([14.438, -9.418], dtype=float32), observed_return=array([15.1, -9. ], dtype=float32), horizon=9, next_observation=array([0, 0], dtype=int32), prob=0.20648557, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([14.87 , -8.068], dtype=float32), observed_return=array([15.1, -8. ], dtype=float32), horizon=8, next_observation=array([0, 1], dtype=int32), prob=0.5694742, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([15.179, -6.774], dtype=float32), observed_return=array([15.1, -7. ], dtype=float32), horizon=7, next_observation=array([0, 2], dtype=int32), prob=0.34005427, terminal=False),\n","   Transition(observation=array([0, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([15.333, -5.678], dtype=float32), observed_return=array([15.1, -6. ], dtype=float32), horizon=6, next_observation=array([1, 2], dtype=int32), prob=0.62727714, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([15.378, -4.5  ], dtype=float32), observed_return=array([15.1, -5. ], dtype=float32), horizon=5, next_observation=array([1, 3], dtype=int32), prob=0.20444742, terminal=False),\n","   Transition(observation=array([1, 3], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([15.346, -3.573], dtype=float32), observed_return=array([15.1, -4. ], dtype=float32), horizon=4, next_observation=array([2, 3], dtype=int32), prob=0.7928672, terminal=False),\n","   Transition(observation=array([2, 3], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([15.285, -2.72 ], dtype=float32), observed_return=array([15.1, -3. ], dtype=float32), horizon=3, next_observation=array([3, 3], dtype=int32), prob=0.6311139, terminal=False),\n","   Transition(observation=array([3, 3], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([15.211, -1.892], dtype=float32), observed_return=array([15.1, -2. ], dtype=float32), horizon=2, next_observation=array([3, 4], dtype=int32), prob=0.21402633, terminal=False),\n","   Transition(observation=array([3, 4], dtype=int32), action=1, reward=array([15.1, -1. ], dtype=float32), return_=array([15.1, -1. ], dtype=float32), observed_return=array([15.1, -1. ], dtype=float32), horizon=1, next_observation=array([4, 4], dtype=int32), prob=0.6621799, terminal=True)]),\n"," (-2e-05,\n","  3539,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.23167515, terminal=True)]),\n"," (-0.0,\n","  7630,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([14.463, -9.975], dtype=float32), observed_return=array([ 15.1, -10. ], dtype=float32), horizon=10, next_observation=array([0, 1], dtype=int32), prob=0.54725975, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([14.593, -8.745], dtype=float32), observed_return=array([15.1, -9. ], dtype=float32), horizon=9, next_observation=array([0, 2], dtype=int32), prob=0.30872348, terminal=False),\n","   Transition(observation=array([0, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([14.614, -7.628], dtype=float32), observed_return=array([15.1, -8. ], dtype=float32), horizon=8, next_observation=array([1, 2], dtype=int32), prob=0.5287406, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([14.65 , -6.569], dtype=float32), observed_return=array([15.1, -7. ], dtype=float32), horizon=7, next_observation=array([2, 2], dtype=int32), prob=0.39893687, terminal=False),\n","   Transition(observation=array([2, 2], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([14.649, -5.518], dtype=float32), observed_return=array([15.1, -6. ], dtype=float32), horizon=6, next_observation=array([2, 3], dtype=int32), prob=0.41690594, terminal=False),\n","   Transition(observation=array([2, 3], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([14.669, -4.56 ], dtype=float32), observed_return=array([15.1, -5. ], dtype=float32), horizon=5, next_observation=array([3, 3], dtype=int32), prob=0.4906116, terminal=False),\n","   Transition(observation=array([3, 3], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([14.739, -3.751], dtype=float32), observed_return=array([15.1, -4. ], dtype=float32), horizon=4, next_observation=array([2, 3], dtype=int32), prob=0.18375187, terminal=False),\n","   Transition(observation=array([2, 3], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([14.837, -2.83 ], dtype=float32), observed_return=array([15.1, -3. ], dtype=float32), horizon=3, next_observation=array([3, 3], dtype=int32), prob=0.4423412, terminal=False),\n","   Transition(observation=array([3, 3], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([14.97 , -1.967], dtype=float32), observed_return=array([15.1, -2. ], dtype=float32), horizon=2, next_observation=array([3, 4], dtype=int32), prob=0.37757054, terminal=False),\n","   Transition(observation=array([3, 4], dtype=int32), action=1, reward=array([15.1, -1. ], dtype=float32), return_=array([15.1, -1. ], dtype=float32), observed_return=array([15.1, -1. ], dtype=float32), horizon=1, next_observation=array([4, 4], dtype=int32), prob=0.5883825, terminal=True)]),\n"," (-2e-05,\n","  8700,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.31706542, terminal=True)]),\n"," (-2e-05,\n","  12639,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.30225348, terminal=True)]),\n"," (-2e-05,\n","  3864,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.18680672, terminal=True)]),\n"," (-2e-05,\n","  1615,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.2869926, terminal=True)]),\n"," (-2e-05,\n","  13342,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.26320803, terminal=True)]),\n"," (-2e-05,\n","  5042,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.17435206, terminal=True)]),\n"," (-2e-05,\n","  12271,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.3012532, terminal=True)]),\n"," (-0.0,\n","  5031,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.518, -4.593], dtype=float32), observed_return=array([ 8.2, -5. ], dtype=float32), horizon=5, next_observation=array([0, 1], dtype=int32), prob=0.38281527, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.474, -3.569], dtype=float32), observed_return=array([ 8.2, -4. ], dtype=float32), horizon=4, next_observation=array([1, 1], dtype=int32), prob=0.25121346, terminal=False),\n","   Transition(observation=array([1, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.395, -2.661], dtype=float32), observed_return=array([ 8.2, -3. ], dtype=float32), horizon=3, next_observation=array([1, 2], dtype=int32), prob=0.29996517, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 8.309, -1.816], dtype=float32), observed_return=array([ 8.2, -2. ], dtype=float32), horizon=2, next_observation=array([2, 2], dtype=int32), prob=0.25481474, terminal=False),\n","   Transition(observation=array([2, 2], dtype=int32), action=2, reward=array([ 8.2, -1. ], dtype=float32), return_=array([ 8.2, -1. ], dtype=float32), observed_return=array([ 8.2, -1. ], dtype=float32), horizon=1, next_observation=array([2, 1], dtype=int32), prob=0.44263777, terminal=True)]),\n"," (-0.0,\n","  1649,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.816, -2.937], dtype=float32), observed_return=array([ 0.7, -3. ], dtype=float32), horizon=3, next_observation=array([0, 1], dtype=int32), prob=0.2637938, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 0.731, -1.984], dtype=float32), observed_return=array([ 0.7, -2. ], dtype=float32), horizon=2, next_observation=array([1, 1], dtype=int32), prob=0.36763367, terminal=False),\n","   Transition(observation=array([1, 1], dtype=int32), action=2, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.21135901, terminal=True)]),\n"," (-2e-05,\n","  8717,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.31706542, terminal=True)]),\n"," (-2e-05,\n","  8292,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.14252363, terminal=True)]),\n"," (-2e-05,\n","  6137,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.17875856, terminal=True)]),\n"," (-0.0,\n","  9691,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 19.488, -48.436], dtype=float32), observed_return=array([ 23.7, -59. ], dtype=float32), horizon=59, next_observation=array([0, 1], dtype=int32), prob=0.3793939, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 19.705, -47.368], dtype=float32), observed_return=array([ 23.7, -58. ], dtype=float32), horizon=58, next_observation=array([0, 1], dtype=int32), prob=0.27093542, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 19.94, -46.25], dtype=float32), observed_return=array([ 23.7, -57. ], dtype=float32), horizon=57, next_observation=array([0, 2], dtype=int32), prob=0.41461998, terminal=False),\n","   Transition(observation=array([0, 2], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.082, -45.408], dtype=float32), observed_return=array([ 23.7, -56. ], dtype=float32), horizon=56, next_observation=array([0, 2], dtype=int32), prob=0.28872806, terminal=False),\n","   Transition(observation=array([0, 2], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.226, -44.566], dtype=float32), observed_return=array([ 23.7, -55. ], dtype=float32), horizon=55, next_observation=array([0, 3], dtype=int32), prob=0.3624852, terminal=False),\n","   Transition(observation=array([0, 3], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.32 , -43.823], dtype=float32), observed_return=array([ 23.7, -54. ], dtype=float32), horizon=54, next_observation=array([0, 4], dtype=int32), prob=0.29979652, terminal=False),\n","   Transition(observation=array([0, 4], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.397, -43.095], dtype=float32), observed_return=array([ 23.7, -53. ], dtype=float32), horizon=53, next_observation=array([0, 5], dtype=int32), prob=0.27103546, terminal=False),\n","   Transition(observation=array([0, 5], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.472, -42.37 ], dtype=float32), observed_return=array([ 23.7, -52. ], dtype=float32), horizon=52, next_observation=array([0, 4], dtype=int32), prob=0.32531562, terminal=False),\n","   Transition(observation=array([0, 4], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.549, -41.634], dtype=float32), observed_return=array([ 23.7, -51. ], dtype=float32), horizon=51, next_observation=array([0, 4], dtype=int32), prob=0.268709, terminal=False),\n","   Transition(observation=array([0, 4], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.627, -40.891], dtype=float32), observed_return=array([ 23.7, -50. ], dtype=float32), horizon=50, next_observation=array([0, 3], dtype=int32), prob=0.25717315, terminal=False),\n","   Transition(observation=array([0, 3], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.711, -40.161], dtype=float32), observed_return=array([ 23.7, -49. ], dtype=float32), horizon=49, next_observation=array([1, 3], dtype=int32), prob=0.19179378, terminal=False),\n","   Transition(observation=array([1, 3], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.784, -39.752], dtype=float32), observed_return=array([ 23.7, -48. ], dtype=float32), horizon=48, next_observation=array([1, 4], dtype=int32), prob=0.3406885, terminal=False),\n","   Transition(observation=array([1, 4], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.849, -39.34 ], dtype=float32), observed_return=array([ 23.7, -47. ], dtype=float32), horizon=47, next_observation=array([0, 4], dtype=int32), prob=0.30732712, terminal=False),\n","   Transition(observation=array([0, 4], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.93 , -38.641], dtype=float32), observed_return=array([ 23.7, -46. ], dtype=float32), horizon=46, next_observation=array([1, 4], dtype=int32), prob=0.20763886, terminal=False),\n","   Transition(observation=array([1, 4], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.998, -38.223], dtype=float32), observed_return=array([ 23.7, -45. ], dtype=float32), horizon=45, next_observation=array([1, 5], dtype=int32), prob=0.30477586, terminal=False),\n","   Transition(observation=array([1, 5], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 21.076, -37.752], dtype=float32), observed_return=array([ 23.7, -44. ], dtype=float32), horizon=44, next_observation=array([1, 6], dtype=int32), prob=0.29460803, terminal=False),\n","   Transition(observation=array([1, 6], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 21.167, -37.231], dtype=float32), observed_return=array([ 23.7, -43. ], dtype=float32), horizon=43, next_observation=array([2, 6], dtype=int32), prob=0.23848978, terminal=False),\n","   Transition(observation=array([2, 6], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 21.218, -36.699], dtype=float32), observed_return=array([ 23.7, -42. ], dtype=float32), horizon=42, next_observation=array([2, 7], dtype=int32), prob=0.28376362, terminal=False),\n","   Transition(observation=array([2, 7], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 21.243, -35.96 ], dtype=float32), observed_return=array([ 23.7, -41. ], dtype=float32), horizon=41, next_observation=array([2, 8], dtype=int32), prob=0.29653037, terminal=False),\n","   Transition(observation=array([2, 8], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 21.249, -35.053], dtype=float32), observed_return=array([ 23.7, -40. ], dtype=float32), horizon=40, next_observation=array([1, 8], dtype=int32), prob=0.2925501, terminal=False),\n","   Transition(observation=array([1, 8], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 21.273, -34.126], dtype=float32), observed_return=array([ 23.7, -39. ], dtype=float32), horizon=39, next_observation=array([2, 8], dtype=int32), prob=0.2528774, terminal=False),\n","   Transition(observation=array([2, 8], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 21.263, -33.247], dtype=float32), observed_return=array([ 23.7, -38. ], dtype=float32), horizon=38, next_observation=array([3, 8], dtype=int32), prob=0.26424947, terminal=False),\n","   Transition(observation=array([3, 8], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 21.23 , -32.446], dtype=float32), observed_return=array([ 23.7, -37. ], dtype=float32), horizon=37, next_observation=array([2, 8], dtype=int32), prob=0.33783057, terminal=False),\n","   Transition(observation=array([2, 8], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 21.194, -31.675], dtype=float32), observed_return=array([ 23.7, -36. ], dtype=float32), horizon=36, next_observation=array([2, 9], dtype=int32), prob=0.27669173, terminal=False),\n","   Transition(observation=array([2, 9], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 21.139, -30.856], dtype=float32), observed_return=array([ 23.7, -35. ], dtype=float32), horizon=35, next_observation=array([1, 9], dtype=int32), prob=0.2657, terminal=False),\n","   Transition(observation=array([1, 9], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 21.085, -30.063], dtype=float32), observed_return=array([ 23.7, -34. ], dtype=float32), horizon=34, next_observation=array([ 1, 10], dtype=int32), prob=0.3426206, terminal=False),\n","   Transition(observation=array([ 1, 10], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 21.019, -29.223], dtype=float32), observed_return=array([ 23.7, -33. ], dtype=float32), horizon=33, next_observation=array([ 1, 10], dtype=int32), prob=0.3692479, terminal=False),\n","   Transition(observation=array([ 1, 10], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.957, -28.394], dtype=float32), observed_return=array([ 23.7, -32. ], dtype=float32), horizon=32, next_observation=array([ 1, 10], dtype=int32), prob=0.36984968, terminal=False),\n","   Transition(observation=array([ 1, 10], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.902, -27.562], dtype=float32), observed_return=array([ 23.7, -31. ], dtype=float32), horizon=31, next_observation=array([1, 9], dtype=int32), prob=0.17993332, terminal=False),\n","   Transition(observation=array([1, 9], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.863, -26.755], dtype=float32), observed_return=array([ 23.7, -30. ], dtype=float32), horizon=30, next_observation=array([1, 8], dtype=int32), prob=0.1931852, terminal=False),\n","   Transition(observation=array([1, 8], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.838, -25.958], dtype=float32), observed_return=array([ 23.7, -29. ], dtype=float32), horizon=29, next_observation=array([1, 7], dtype=int32), prob=0.20286009, terminal=False),\n","   Transition(observation=array([1, 7], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.825, -25.177], dtype=float32), observed_return=array([ 23.7, -28. ], dtype=float32), horizon=28, next_observation=array([1, 8], dtype=int32), prob=0.33114177, terminal=False),\n","   Transition(observation=array([1, 8], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.803, -24.319], dtype=float32), observed_return=array([ 23.7, -27. ], dtype=float32), horizon=27, next_observation=array([1, 9], dtype=int32), prob=0.3453453, terminal=False),\n","   Transition(observation=array([1, 9], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.777, -23.396], dtype=float32), observed_return=array([ 23.7, -26. ], dtype=float32), horizon=26, next_observation=array([ 1, 10], dtype=int32), prob=0.34638935, terminal=False),\n","   Transition(observation=array([ 1, 10], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.744, -22.429], dtype=float32), observed_return=array([ 23.7, -25. ], dtype=float32), horizon=25, next_observation=array([1, 9], dtype=int32), prob=0.20018698, terminal=False),\n","   Transition(observation=array([1, 9], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.692, -21.545], dtype=float32), observed_return=array([ 23.7, -24. ], dtype=float32), horizon=24, next_observation=array([ 1, 10], dtype=int32), prob=0.3194618, terminal=False),\n","   Transition(observation=array([ 1, 10], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.624, -20.673], dtype=float32), observed_return=array([ 23.7, -23. ], dtype=float32), horizon=23, next_observation=array([ 1, 10], dtype=int32), prob=0.32770756, terminal=False),\n","   Transition(observation=array([ 1, 10], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.541, -19.835], dtype=float32), observed_return=array([ 23.7, -22. ], dtype=float32), horizon=22, next_observation=array([ 2, 10], dtype=int32), prob=0.29832718, terminal=False),\n","   Transition(observation=array([ 2, 10], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.445, -18.998], dtype=float32), observed_return=array([ 23.7, -21. ], dtype=float32), horizon=21, next_observation=array([ 3, 10], dtype=int32), prob=0.2800051, terminal=False),\n","   Transition(observation=array([ 3, 10], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.345, -18.127], dtype=float32), observed_return=array([ 23.7, -20. ], dtype=float32), horizon=20, next_observation=array([ 4, 10], dtype=int32), prob=0.32868338, terminal=False),\n","   Transition(observation=array([ 4, 10], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.256, -17.271], dtype=float32), observed_return=array([ 23.7, -19. ], dtype=float32), horizon=19, next_observation=array([4, 9], dtype=int32), prob=0.2268749, terminal=False),\n","   Transition(observation=array([4, 9], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.192, -16.474], dtype=float32), observed_return=array([ 23.7, -18. ], dtype=float32), horizon=18, next_observation=array([4, 8], dtype=int32), prob=0.18503545, terminal=False),\n","   Transition(observation=array([4, 8], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.151, -15.705], dtype=float32), observed_return=array([ 23.7, -17. ], dtype=float32), horizon=17, next_observation=array([4, 9], dtype=int32), prob=0.47621873, terminal=False),\n","   Transition(observation=array([4, 9], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.125, -14.92 ], dtype=float32), observed_return=array([ 23.7, -16. ], dtype=float32), horizon=16, next_observation=array([4, 8], dtype=int32), prob=0.17827524, terminal=False),\n","   Transition(observation=array([4, 8], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.121, -14.127], dtype=float32), observed_return=array([ 23.7, -15. ], dtype=float32), horizon=15, next_observation=array([4, 9], dtype=int32), prob=0.50462365, terminal=False),\n","   Transition(observation=array([4, 9], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.133, -13.301], dtype=float32), observed_return=array([ 23.7, -14. ], dtype=float32), horizon=14, next_observation=array([4, 8], dtype=int32), prob=0.20144868, terminal=False),\n","   Transition(observation=array([4, 8], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.179, -12.496], dtype=float32), observed_return=array([ 23.7, -13. ], dtype=float32), horizon=13, next_observation=array([3, 8], dtype=int32), prob=0.17654139, terminal=False),\n","   Transition(observation=array([3, 8], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.245, -11.682], dtype=float32), observed_return=array([ 23.7, -12. ], dtype=float32), horizon=12, next_observation=array([3, 9], dtype=int32), prob=0.20654322, terminal=False),\n","   Transition(observation=array([3, 9], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.333, -10.864], dtype=float32), observed_return=array([ 23.7, -11. ], dtype=float32), horizon=11, next_observation=array([4, 9], dtype=int32), prob=0.70096797, terminal=False),\n","   Transition(observation=array([4, 9], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.464, -10.085], dtype=float32), observed_return=array([ 23.7, -10. ], dtype=float32), horizon=10, next_observation=array([5, 9], dtype=int32), prob=0.5677869, terminal=False),\n","   Transition(observation=array([5, 9], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([20.625, -9.287], dtype=float32), observed_return=array([23.7, -9. ], dtype=float32), horizon=9, next_observation=array([ 5, 10], dtype=int32), prob=0.20534682, terminal=False),\n","   Transition(observation=array([ 5, 10], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([20.804, -8.417], dtype=float32), observed_return=array([23.7, -8. ], dtype=float32), horizon=8, next_observation=array([ 6, 10], dtype=int32), prob=0.6758964, terminal=False),\n","   Transition(observation=array([ 6, 10], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([21.028, -7.51 ], dtype=float32), observed_return=array([23.7, -7. ], dtype=float32), horizon=7, next_observation=array([ 5, 10], dtype=int32), prob=0.010626074, terminal=False),\n","   Transition(observation=array([ 5, 10], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([21.263, -6.47 ], dtype=float32), observed_return=array([23.7, -6. ], dtype=float32), horizon=6, next_observation=array([ 6, 10], dtype=int32), prob=0.82153875, terminal=False),\n","   Transition(observation=array([ 6, 10], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([21.608, -5.505], dtype=float32), observed_return=array([23.7, -5. ], dtype=float32), horizon=5, next_observation=array([6, 9], dtype=int32), prob=0.16524093, terminal=False),\n","   Transition(observation=array([6, 9], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([22.044, -4.536], dtype=float32), observed_return=array([23.7, -4. ], dtype=float32), horizon=4, next_observation=array([7, 9], dtype=int32), prob=0.8344903, terminal=False),\n","   Transition(observation=array([7, 9], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([22.535, -3.473], dtype=float32), observed_return=array([23.7, -3. ], dtype=float32), horizon=3, next_observation=array([8, 9], dtype=int32), prob=0.84540594, terminal=False),\n","   Transition(observation=array([8, 9], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([23.083, -2.296], dtype=float32), observed_return=array([23.7, -2. ], dtype=float32), horizon=2, next_observation=array([9, 9], dtype=int32), prob=0.82358825, terminal=False),\n","   Transition(observation=array([9, 9], dtype=int32), action=1, reward=array([23.7, -1. ], dtype=float32), return_=array([23.7, -1. ], dtype=float32), observed_return=array([23.7, -1. ], dtype=float32), horizon=1, next_observation=array([10,  9], dtype=int32), prob=0.7783365, terminal=True)]),\n"," (-0.0,\n","  11037,\n","  [Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([11.942, -6.877], dtype=float32), observed_return=array([11.5, -7. ], dtype=float32), horizon=7, next_observation=array([0, 1], dtype=int32), prob=0.53128636, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([12.153, -5.66 ], dtype=float32), observed_return=array([11.5, -6. ], dtype=float32), horizon=6, next_observation=array([0, 2], dtype=int32), prob=0.36871368, terminal=False),\n","   Transition(observation=array([0, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([12.202, -4.636], dtype=float32), observed_return=array([11.5, -5. ], dtype=float32), horizon=5, next_observation=array([1, 2], dtype=int32), prob=0.52723664, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([12.152, -3.549], dtype=float32), observed_return=array([11.5, -4. ], dtype=float32), horizon=4, next_observation=array([2, 2], dtype=int32), prob=0.35578442, terminal=False),\n","   Transition(observation=array([2, 2], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([12.001, -2.537], dtype=float32), observed_return=array([11.5, -3. ], dtype=float32), horizon=3, next_observation=array([2, 3], dtype=int32), prob=0.44223458, terminal=False),\n","   Transition(observation=array([2, 3], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([11.781, -1.754], dtype=float32), observed_return=array([11.5, -2. ], dtype=float32), horizon=2, next_observation=array([3, 3], dtype=int32), prob=0.60318995, terminal=False),\n","   Transition(observation=array([3, 3], dtype=int32), action=2, reward=array([11.5, -1. ], dtype=float32), return_=array([11.5, -1. ], dtype=float32), observed_return=array([11.5, -1. ], dtype=float32), horizon=1, next_observation=array([3, 2], dtype=int32), prob=0.29827636, terminal=True)]),\n"," (-0.0,\n","  121,\n","  [Transition(observation=array([0, 0], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -86. ], dtype=float32), observed_return=array([ 20.3, -86. ], dtype=float32), horizon=86, next_observation=array([0, 0], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 0], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -85. ], dtype=float32), observed_return=array([ 20.3, -85. ], dtype=float32), horizon=85, next_observation=array([0, 1], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 1], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -84. ], dtype=float32), observed_return=array([ 20.3, -84. ], dtype=float32), horizon=84, next_observation=array([1, 1], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([1, 1], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -83. ], dtype=float32), observed_return=array([ 20.3, -83. ], dtype=float32), horizon=83, next_observation=array([1, 2], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([1, 2], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -82. ], dtype=float32), observed_return=array([ 20.3, -82. ], dtype=float32), horizon=82, next_observation=array([1, 3], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([1, 3], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -81. ], dtype=float32), observed_return=array([ 20.3, -81. ], dtype=float32), horizon=81, next_observation=array([0, 3], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 3], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -80. ], dtype=float32), observed_return=array([ 20.3, -80. ], dtype=float32), horizon=80, next_observation=array([0, 4], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 4], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -79. ], dtype=float32), observed_return=array([ 20.3, -79. ], dtype=float32), horizon=79, next_observation=array([0, 5], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 5], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -78. ], dtype=float32), observed_return=array([ 20.3, -78. ], dtype=float32), horizon=78, next_observation=array([0, 5], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 5], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -77. ], dtype=float32), observed_return=array([ 20.3, -77. ], dtype=float32), horizon=77, next_observation=array([0, 4], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 4], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -76. ], dtype=float32), observed_return=array([ 20.3, -76. ], dtype=float32), horizon=76, next_observation=array([0, 3], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 3], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -75. ], dtype=float32), observed_return=array([ 20.3, -75. ], dtype=float32), horizon=75, next_observation=array([0, 2], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 2], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -74. ], dtype=float32), observed_return=array([ 20.3, -74. ], dtype=float32), horizon=74, next_observation=array([0, 2], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 2], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -73. ], dtype=float32), observed_return=array([ 20.3, -73. ], dtype=float32), horizon=73, next_observation=array([0, 3], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 3], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -72. ], dtype=float32), observed_return=array([ 20.3, -72. ], dtype=float32), horizon=72, next_observation=array([1, 3], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([1, 3], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -71. ], dtype=float32), observed_return=array([ 20.3, -71. ], dtype=float32), horizon=71, next_observation=array([1, 4], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([1, 4], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -70. ], dtype=float32), observed_return=array([ 20.3, -70. ], dtype=float32), horizon=70, next_observation=array([1, 5], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([1, 5], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -69. ], dtype=float32), observed_return=array([ 20.3, -69. ], dtype=float32), horizon=69, next_observation=array([1, 6], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([1, 6], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -68. ], dtype=float32), observed_return=array([ 20.3, -68. ], dtype=float32), horizon=68, next_observation=array([2, 6], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([2, 6], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -67. ], dtype=float32), observed_return=array([ 20.3, -67. ], dtype=float32), horizon=67, next_observation=array([2, 7], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([2, 7], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -66. ], dtype=float32), observed_return=array([ 20.3, -66. ], dtype=float32), horizon=66, next_observation=array([2, 8], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([2, 8], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -65. ], dtype=float32), observed_return=array([ 20.3, -65. ], dtype=float32), horizon=65, next_observation=array([1, 8], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([1, 8], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -64. ], dtype=float32), observed_return=array([ 20.3, -64. ], dtype=float32), horizon=64, next_observation=array([2, 8], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([2, 8], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -63. ], dtype=float32), observed_return=array([ 20.3, -63. ], dtype=float32), horizon=63, next_observation=array([3, 8], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([3, 8], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -62. ], dtype=float32), observed_return=array([ 20.3, -62. ], dtype=float32), horizon=62, next_observation=array([4, 8], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([4, 8], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -61. ], dtype=float32), observed_return=array([ 20.3, -61. ], dtype=float32), horizon=61, next_observation=array([3, 8], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([3, 8], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -60. ], dtype=float32), observed_return=array([ 20.3, -60. ], dtype=float32), horizon=60, next_observation=array([4, 8], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([4, 8], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -59. ], dtype=float32), observed_return=array([ 20.3, -59. ], dtype=float32), horizon=59, next_observation=array([5, 8], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([5, 8], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -58. ], dtype=float32), observed_return=array([ 20.3, -58. ], dtype=float32), horizon=58, next_observation=array([5, 7], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([5, 7], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -57. ], dtype=float32), observed_return=array([ 20.3, -57. ], dtype=float32), horizon=57, next_observation=array([6, 7], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([6, 7], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -56. ], dtype=float32), observed_return=array([ 20.3, -56. ], dtype=float32), horizon=56, next_observation=array([5, 7], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([5, 7], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -55. ], dtype=float32), observed_return=array([ 20.3, -55. ], dtype=float32), horizon=55, next_observation=array([4, 7], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([4, 7], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -54. ], dtype=float32), observed_return=array([ 20.3, -54. ], dtype=float32), horizon=54, next_observation=array([3, 7], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([3, 7], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -53. ], dtype=float32), observed_return=array([ 20.3, -53. ], dtype=float32), horizon=53, next_observation=array([4, 7], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([4, 7], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -52. ], dtype=float32), observed_return=array([ 20.3, -52. ], dtype=float32), horizon=52, next_observation=array([5, 7], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([5, 7], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -51. ], dtype=float32), observed_return=array([ 20.3, -51. ], dtype=float32), horizon=51, next_observation=array([4, 7], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([4, 7], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -50. ], dtype=float32), observed_return=array([ 20.3, -50. ], dtype=float32), horizon=50, next_observation=array([4, 8], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([4, 8], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -49. ], dtype=float32), observed_return=array([ 20.3, -49. ], dtype=float32), horizon=49, next_observation=array([4, 9], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([4, 9], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -48. ], dtype=float32), observed_return=array([ 20.3, -48. ], dtype=float32), horizon=48, next_observation=array([5, 9], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([5, 9], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -47. ], dtype=float32), observed_return=array([ 20.3, -47. ], dtype=float32), horizon=47, next_observation=array([ 5, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 5, 10], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -46. ], dtype=float32), observed_return=array([ 20.3, -46. ], dtype=float32), horizon=46, next_observation=array([ 4, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 4, 10], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -45. ], dtype=float32), observed_return=array([ 20.3, -45. ], dtype=float32), horizon=45, next_observation=array([ 3, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 3, 10], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -44. ], dtype=float32), observed_return=array([ 20.3, -44. ], dtype=float32), horizon=44, next_observation=array([ 4, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 4, 10], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -43. ], dtype=float32), observed_return=array([ 20.3, -43. ], dtype=float32), horizon=43, next_observation=array([ 4, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 4, 10], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -42. ], dtype=float32), observed_return=array([ 20.3, -42. ], dtype=float32), horizon=42, next_observation=array([ 4, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 4, 10], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -41. ], dtype=float32), observed_return=array([ 20.3, -41. ], dtype=float32), horizon=41, next_observation=array([ 3, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 3, 10], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -40. ], dtype=float32), observed_return=array([ 20.3, -40. ], dtype=float32), horizon=40, next_observation=array([ 4, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 4, 10], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -39. ], dtype=float32), observed_return=array([ 20.3, -39. ], dtype=float32), horizon=39, next_observation=array([ 3, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 3, 10], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -38. ], dtype=float32), observed_return=array([ 20.3, -38. ], dtype=float32), horizon=38, next_observation=array([ 2, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 2, 10], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -37. ], dtype=float32), observed_return=array([ 20.3, -37. ], dtype=float32), horizon=37, next_observation=array([2, 9], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([2, 9], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -36. ], dtype=float32), observed_return=array([ 20.3, -36. ], dtype=float32), horizon=36, next_observation=array([2, 8], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([2, 8], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -35. ], dtype=float32), observed_return=array([ 20.3, -35. ], dtype=float32), horizon=35, next_observation=array([1, 8], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([1, 8], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -34. ], dtype=float32), observed_return=array([ 20.3, -34. ], dtype=float32), horizon=34, next_observation=array([1, 9], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([1, 9], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -33. ], dtype=float32), observed_return=array([ 20.3, -33. ], dtype=float32), horizon=33, next_observation=array([0, 9], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 9], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -32. ], dtype=float32), observed_return=array([ 20.3, -32. ], dtype=float32), horizon=32, next_observation=array([ 0, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 0, 10], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -31. ], dtype=float32), observed_return=array([ 20.3, -31. ], dtype=float32), horizon=31, next_observation=array([ 1, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 1, 10], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -30. ], dtype=float32), observed_return=array([ 20.3, -30. ], dtype=float32), horizon=30, next_observation=array([ 0, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 0, 10], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -29. ], dtype=float32), observed_return=array([ 20.3, -29. ], dtype=float32), horizon=29, next_observation=array([ 0, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 0, 10], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -28. ], dtype=float32), observed_return=array([ 20.3, -28. ], dtype=float32), horizon=28, next_observation=array([ 1, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 1, 10], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -27. ], dtype=float32), observed_return=array([ 20.3, -27. ], dtype=float32), horizon=27, next_observation=array([ 0, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 0, 10], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -26. ], dtype=float32), observed_return=array([ 20.3, -26. ], dtype=float32), horizon=26, next_observation=array([0, 9], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 9], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -25. ], dtype=float32), observed_return=array([ 20.3, -25. ], dtype=float32), horizon=25, next_observation=array([ 0, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 0, 10], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -24. ], dtype=float32), observed_return=array([ 20.3, -24. ], dtype=float32), horizon=24, next_observation=array([ 0, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 0, 10], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -23. ], dtype=float32), observed_return=array([ 20.3, -23. ], dtype=float32), horizon=23, next_observation=array([0, 9], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 9], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -22. ], dtype=float32), observed_return=array([ 20.3, -22. ], dtype=float32), horizon=22, next_observation=array([0, 8], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 8], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -21. ], dtype=float32), observed_return=array([ 20.3, -21. ], dtype=float32), horizon=21, next_observation=array([0, 9], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([0, 9], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -20. ], dtype=float32), observed_return=array([ 20.3, -20. ], dtype=float32), horizon=20, next_observation=array([1, 9], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([1, 9], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -19. ], dtype=float32), observed_return=array([ 20.3, -19. ], dtype=float32), horizon=19, next_observation=array([2, 9], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([2, 9], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -18. ], dtype=float32), observed_return=array([ 20.3, -18. ], dtype=float32), horizon=18, next_observation=array([3, 9], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([3, 9], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -17. ], dtype=float32), observed_return=array([ 20.3, -17. ], dtype=float32), horizon=17, next_observation=array([4, 9], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([4, 9], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -16. ], dtype=float32), observed_return=array([ 20.3, -16. ], dtype=float32), horizon=16, next_observation=array([ 4, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 4, 10], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -15. ], dtype=float32), observed_return=array([ 20.3, -15. ], dtype=float32), horizon=15, next_observation=array([ 5, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 5, 10], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -14. ], dtype=float32), observed_return=array([ 20.3, -14. ], dtype=float32), horizon=14, next_observation=array([ 5, 10], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([ 5, 10], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -13. ], dtype=float32), observed_return=array([ 20.3, -13. ], dtype=float32), horizon=13, next_observation=array([5, 9], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([5, 9], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -12. ], dtype=float32), observed_return=array([ 20.3, -12. ], dtype=float32), horizon=12, next_observation=array([5, 8], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([5, 8], dtype=int32), action=0, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -11. ], dtype=float32), observed_return=array([ 20.3, -11. ], dtype=float32), horizon=11, next_observation=array([4, 8], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([4, 8], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([ 20.3, -10. ], dtype=float32), observed_return=array([ 20.3, -10. ], dtype=float32), horizon=10, next_observation=array([4, 9], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([4, 9], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([20.3, -9. ], dtype=float32), observed_return=array([20.3, -9. ], dtype=float32), horizon=9, next_observation=array([4, 8], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([4, 8], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([20.3, -8. ], dtype=float32), observed_return=array([20.3, -8. ], dtype=float32), horizon=8, next_observation=array([5, 8], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([5, 8], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([20.3, -7. ], dtype=float32), observed_return=array([20.3, -7. ], dtype=float32), horizon=7, next_observation=array([5, 9], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([5, 9], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([20.3, -6. ], dtype=float32), observed_return=array([20.3, -6. ], dtype=float32), horizon=6, next_observation=array([5, 8], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([5, 8], dtype=int32), action=3, reward=array([ 0., -1.], dtype=float32), return_=array([20.3, -5. ], dtype=float32), observed_return=array([20.3, -5. ], dtype=float32), horizon=5, next_observation=array([5, 9], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([5, 9], dtype=int32), action=1, reward=array([ 0., -1.], dtype=float32), return_=array([20.3, -4. ], dtype=float32), observed_return=array([20.3, -4. ], dtype=float32), horizon=4, next_observation=array([6, 9], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([6, 9], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([20.3, -3. ], dtype=float32), observed_return=array([20.3, -3. ], dtype=float32), horizon=3, next_observation=array([6, 8], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([6, 8], dtype=int32), action=2, reward=array([ 0., -1.], dtype=float32), return_=array([20.3, -2. ], dtype=float32), observed_return=array([20.3, -2. ], dtype=float32), horizon=2, next_observation=array([6, 7], dtype=int32), prob=0.25, terminal=False),\n","   Transition(observation=array([6, 7], dtype=int32), action=1, reward=array([20.3, -1. ], dtype=float32), return_=array([20.3, -1. ], dtype=float32), observed_return=array([20.3, -1. ], dtype=float32), horizon=1, next_observation=array([7, 7], dtype=int32), prob=0.25, terminal=True)]),\n"," (-2e-05,\n","  4288,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.1560213, terminal=True)]),\n"," (-1e-05,\n","  6900,\n","  [Transition(observation=array([0, 0], dtype=int32), action=1, reward=array([ 0.7, -1. ], dtype=float32), return_=array([ 0.7, -1. ], dtype=float32), observed_return=array([ 0.7, -1. ], dtype=float32), horizon=1, next_observation=array([1, 0], dtype=int32), prob=0.19557023, terminal=True)])]"]},"metadata":{},"execution_count":3}],"source":["agent.experience_replay"]},{"cell_type":"markdown","metadata":{"id":"e5LPy1SReVGw"},"source":["# Training Notes\n","- Most episodes in the ER only reach the first reward:\n","  - ER buffer is constructed such that new episodes are always added\n","  - This removes episodes by:\n","    1. Duplicates\n","    2. Distance metric (L2 + crowding)\n","    3. Oldest first\n","  - better returns are achieved as better PF returns are added to the PF\n","\n","- buffer size, batch size and num expl episodes are important choices:\n","  - Num expl episodes needs to add enough episodes for them to be of significance during the next training iteration(s)\n","  - If num_exl is too large (and buffer size too small) then exploration episodes can overwrite episodes with good returns!\n","  - The smaller the batch size, the more these episodes have impact on training => faster convergence (depending on environment0\n","  - Larger buffer size means less chance of these exploration episodes to be used for training in the next timestep. A sufficient size is needed to ensure enough points are in te buffer for computing each point in the pf (at least batch_size*num_pf_points, but pref. more)\n","  - Preferably small batch size (16/32/64) and large buffer (1024/2048) depending on problem size\n","  - Depending on buffer size, pick sufficiently large number of exploration episodes to increase convergence speed\n","\n","Idea:\n","- In order to improve the PF over time, average returns (from s0) must improve\n","- We need sufficient 'better' trajectories (at least batch_size) for this average to improve\n","- Take batches of (s,a) instead of only over state. This implies that taking similar actions in similar states leads to similar polices, which in term helps with convergence\n","- Using such batches could also solve loss/gradient issues!\n","- This complies with using this model for stochastic transitions, as we want averages over state action pairs\n","- This implies that we learn deterministic policies\n","\n","Questions:\n","- Policy explorations improved drastically after using tanh inbetween hidden layers\n","- Value estimates are more accurate when using tanh\n","- Weighting losses using IS ratio should help because data is very off-policy compared to single-objective RL.\n","- Try to modify V such that:\n","  - V is conditioned on (s,a,R,h), with R being observed returns and outputs expected returns. This way, V learns the relation between returns and expected returns.\n","  - The value loss function should not be weighted this way\n","  - The TD(λ) return needs to be modified slightly => no keeping track of followed value vector, but rather keep track of the MC return (this way we can feed this return into V to get that part of the return)\n","- OR: try to decrease TD(λ) such that expectation is learned implicitly over time instead of by batching\n","- The MC return and the value function have a mismatch (observed vs expected returns), this can become a problem in random environments!\n","- Add stdev used for exploration as param => encourage exploration when only a single point is in the PF\n","\n","TODO:\n","- After policy evaluation it is clear that the agent often does not reach a terminal state => it learns to make cycles and uses these bootstrapped estimates to get its rewards...\n","  - weight policy loss so off-policy actions contribute less => less-likely to learn wrong actions by off policy learning... upon policy eval, the policies result in the first reward only\n","  - Experiment with Beta: slightly larger values result in better policy eval => small beta penalizes steepens difference between expected value and desired policy return. Because of the initial mismatch between the TD-return and the expected returns, these losses are inaccurate\n","\n","- Try to balance actor and critic updates:\n","  - In AWR, the actor is updated more than the critic. This is because the critic is only used to compute the advantage weights, and not for guiding the policy gradients themselves.\n","  - AWR remains stable without any target network for the critic for this same reason. In our case, value estimates are used for computing the TD(lambda) return (as in AWR). But this return is then used as conditioning for both the actor and critic.\n","\n","- PF contains 'intermediate' returns: averages of different solutions which are non-dominated by themselves, but are not a distinct solution when evaluated.\n","  - This is problem specific...\n","  - Eg. the 3 returns next to each other are very close together (thus have a low crowding distance). Expected returns between 0.7 and 8.3 are in the PF, but not achievable (unless by stochastic policy). This is because crowding distance is higher than for the others.\n","  - We can replace the PF points by their evaluated returns (taking Pareto dominance into account).\n","    - ISSUE: mismatch between learned returns, PF and ER returns: picking a pf return and improving it by a slight amount can become impossible due to the policy possibly never having learned such returns... => try to add more expl episodes\n","    - Another issue is that we stop learning to condition on expected returns and replace them by learning from observations!\n","    - IDEA: evaluate policies multiple times, this way we can get expected return (important in stochastic env). Only keep PF returns when their evaluated return is close to itself (by some threshold for each objective)\n","\n","- Learn a PCS for every state (this way we can handle multiple initial states) (difficult to plot though)\n","\n","- When we want to compute the n-step return, we need to use the correct value vector to give to the value function:\n","  - If we use the one saved as value_vec (in the transitions), then we are not guaranteed to use the correct one. The value vector followed during exploration or a random policy does not match the actual value vector of that policy!\n","  - IDEA: use value function to learn relation between observed return and expected returns\n","    - use mean_r as targets\n","    - weight losses as before\n","    - This implies that each policy is uniquely identifiable by V as a pair of a desired return and a horizon\n","    - compute TD(λ) return from state s as: λ*MC return + (1-λ)V(s,R,h)\n","    - for policy and for advantage weight computation use V(s,R,h) instead of R_hat! This should loosen dependency between averaged returns and learned actions\n","    - The policy needs to be conditioned on expected returns => POPF needs to output correct vectors!\n","- Check if policies are stationary vs non-stationary => computing mean return as target for V might be wrong if we consider non-stationary policies => pick batch such that horizon is same within batch\n","    "]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNovvTKbTa5xApYLCjzRamB"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}